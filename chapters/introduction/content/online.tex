\glsresetall
With the increasing need to protect the privacy of sensitive user data while conducting meaningful data analysis,~\Gls{dp}~\citep{dwork2006calibrating} has become a popular solution.~\Gls{dp} algorithms ensure that the impact of any single data sample on the output is limited, thus safeguarding individual privacy. Several works have obtained~\Gls{dp} learning algorithms for various learning problems in both theory and practice.

However, privacy does not come for free and often leads to a statistical (and sometimes computational) cost. 
The classical solution for non-private~\Gls{pac} learning~\citep{valiant1984theory} is via~\Gls{erm} that computes the best solution on the training data. Several works~\citep{bassily2014private,chaudhuri2011differentially} have shown that incorporating DP into~\Gls{erm} incurs a compulsory statistical cost that depends on the dimension of the problem. In the well-known setting of~\Gls{pac} learning with~\Gls{dp},~\citet{kasiviswanathan2011can} provided the first guarantees that all finite VC classes can be learned with a sample size that grows logarithmically in the size of the class. This line of research was advanced by subsequent works~\citep{beimel2013characterizing,feldman2014sample,beimel2014bounds}, resulting in the findings of~\citet{alon2022private} which established a surprising equivalence between non-private online learning and~Approximate~\Gls{dp}-\Gls{pac} learning.

Unlike the setting of~\Gls{pac} learning, Online learning captures a sequential game between a learner and an adversary. 
The adversary knows everything about the learner's algorithm except its random bits. 
In this work we consider a setting where, for a known hypothesis class \(\hyp\), the adversary chooses a sequence of data points \(\bc{x_1,\ldots,x_t}\) and the target hypothesis \(f^*\in\hyp\) prior to engaging with the learner. Then, the adversary reveals these data points one by one to the learner, who must offer a prediction for each. After each prediction, the adversary reveals the true label for that point. The learner's performance is evaluated by comparing the incurred mistakes against the theoretical minimum that could have been achieved by an optimal hypothesis in hindsight. Known as the \emph{realisable oblivious mistake bound} model, the seminal work of~\citet{littlestone1988learning} showed that i) the number of mistakes incurred by any learner is lower-bounded by the Littlestone dimension~(more precisely, \(\nicefrac{\ldim{\hyp}}{2}\)) of the target class \(\hyp\) and ii) there is an algorithm that makes at most~\(\ldim{\hyp}\) mistakes. This algorithm is commonly referred to as the~\Gls{soa}.

Recall that certain problem classes possess finite Vapnik-Chervonenkis (VC) dimensions but infinite Littlestone dimensions (such as the one-dimensional threshold problem). 
This, together with the equivalence between non-private online learning and ~\Gls{dp}-\Gls{pac} learning~\citep{alon2022private} implies that there exists a fundamental separation between DP-PAC learning and non-private PAC learning. In other words, some learning problems can be solved with vanishing error, as the amount of data increases, in~\Gls{pac} learning but will suffer unbounded error in~\Gls{dp}-\Gls{pac} learning. This implication was first proven for pure~\Gls{dp} by~\citet{feldman2014sample} and later for approximate~\Gls{dp} by~\citet{alon2019private}. With the debate on the sample complexity of approximate~\Gls{dp}-\Gls{pac} learning resolved, we next ask whether a similar gap exists between online learning with~\Gls{dp} and non-private online learning.~\citet{golowich2021littlestone} addressed this by introducing the~\Gls{dpsoa}, which suffers a mistake count, that increases logarithmically with the number of rounds \(T\) compared to a constant error rate in non-private online learning~\citep{littlestone1988learning}. This difference suggests a challenge in~\Gls{dp} online learning, where errors increase indefinitely as the game continues. The question of whether this growing error rate is an unavoidable aspect of DP-online learning was posed as an open question by~\citet{dpopsanyal22a}.

\paragraph{Main Result} In this work, we provide evidence that this additional cost is inevitable. Consider any hypothesis class \(\cH\) and for a learning algorithm \(\alg\). Let \(\bE\bs{\regret_{\alg}}\) be the expected number of mistakes incurred by \(\alg\) and let \(T\) be the total number of rounds for which the game is played. 


We obtain a lower bound on \(\bE\bs{M_{\cA}}\) under some assumptions on the learning algorithm \(\cA\). Informally, we say an algorithm \(\cA\) is \(\conc{\beta}\)~(see~\Cref{defn:beta-conc} for a formal definition) if there is some output sequence that it outputs with probability at least \(1-\beta\) in response to a~\emph{non-distinguishing} input sequence. A~\emph{non-distinguishing} input sequence is a~(possibly repeated) sequence of input data points such that there exists some \(f_1,f_2\in\hyp\) which cannot be distinguished just by observing their output on the non-distinguishing input sequence. We prove a general statement for any hypothesis class in~\Cref{thm:main-finite} but show a informal corollary below.

\begin{corollary}[Informal Corollary of~\Cref{thm:main-finite}]\label{thm:informal-small}
     There exists a hypothesis class \(\cH\) with \(\ldim{\cH}=1\) (see~\Cref{defn:littlestone}), such that for any \(\e, \delta > 0, T \leq \exp(1/(32\delta))\), and any online learner \(\alg\) that is \((\e, \delta)\)-\Gls{dp} and \(0.1\)-concentrated, there is an adversary, such that 
    \begin{equation}
        \E\bs{\regret_{\alg}} =\widetilde\Omega\br{\frac{\log T}{\e}},
        \end{equation}
    where \(\widetilde \Omega\) hides logarithmic factors in \(\e\). For \(T > \exp(1/(32\delta))\), \(\E\bs{\regret_{\alg}} = \widetilde\Omega\br{1 / \delta}\).
\end{corollary}

 While the above result uses a hypothesis class of Littlestone dimension one, our main result in~\Cref{sec:finite_horizon_result} also holds for any hypothesis class, even with Littlestone dimension greater than one. Utilising the \(\point_N\) hypothesis class (see~\Cref{defn:point-class}) in~\Cref{thm:informal-small}, we demonstrate that the minimum number of mistakes a~\Gls{dp} online learner must make is bounded below by a term that increases logarithmically with the time horizon \(T\). 
 This holds if the learning algorithm is concentrated and \(T\) is less than or equal to \(\exp\br{1 / (32\delta)}\). This contrasts with non-private online learning, where the number of mistakes does not increase with \(T\) in hypothesis classes with bounded Littlestone dimension, even if the learner is concentrated.\footnote{All deterministic algorithms are \(\conc{0}\) by definition.} Our result also shows that the analysis of the algorithm of~\citet{golowich2021littlestone}, which shows an upper bound of~\(\Omega\br{\log \br{T/\delta}}\) for~\Gls{dpsoa} is tight as long as \(T\leq\exp\br{1 / (32\delta)}\). However, as illustrated in~\Cref{fig:lower-bound}, this is not a limitation as for larger \(T\), since a simple~\emph{Name \& Shame} algorithm incurs lesser mistake than~\Gls{dpsoa} albeit at vacuous privacy levels (see discussion after~\Cref{thm:main-finite}). 


In fact, the assumption of concentrated learners is not overly restrictive given that known~\Gls{dp}-online learning algorithms exhibit this property, as detailed in~\Cref{sec:example-conc}. Notably, the~\Gls{dpsoa} presented by~\citet{golowich2021littlestone} which is the sole DP online learning method known to achieve a mistake bound \(\bigO{\log\br{T}}\), is concentrated as shown in~\Cref{lem:dpsoa-conc}. This suggests that the lower bound holds for all potential~\Gls{dp} online learning algorithms.


Additionally, we extend our result to another class of~\Gls{dp} online algorithms, which we refer to as~\emph{uniform firing} algorithms, that are in essence juxtaposed to concentrated algorithms.  These algorithms initially select predictors at random until a certain
confidence criterion is met, prompting a switch to a consistent predictorâ€”this transition, or `firing', is determined by the flip of a biased coin (with bias \(p_t\)), where the likelihood of firing increases with each mistake. However, the choice of how \(p_t\) increases and the selection of the predictor upon firing depend on the algorithm's design. For this specific type of algorithms, particularly in the context of learning the \(\point_3\) hypothesis class, Proposition~\ref{prop:firing-lb} establishes a lower bound on mistakes that also grows logarithmically with \(T\).


\Cref{sec:continual} discusses Continual Observation~\citep{dwork2010boosting,chan2011private}, another popular task within sequential~\Gls{dp}. We show that results on~\Gls{dp}~Continual Counters can be used to derive upper bounds in the online learning setting. Nonetheless, it is not clear whether lower bounds for that setting can transferred to~\Gls{dp}-Online learning. In addition, these upper bounds suffer a dependence on the hypothesis class size. 

Finally, we point out that, to the best of our knowledge we are unaware of any algorithms in the literature for pure~\Gls{dp} online learning. Our lower bound in~\Cref{thm:informal-small} immediately provides a lower bound for pure~\Gls{dp}. Similarly, DP Continual Counters provide a method for achieving upper bounds, specifically for the \(\point_N\) classes, albeit with a linear dependency on \(N\). Obtaining tight upper and lower bounds remains an interesting direction for future research.
