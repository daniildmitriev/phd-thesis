% Second part of this thesis is devoted to robust statistics, in particular to high-dimensional algorithmtic robust statistics. 

\subsection{Robust mean estimation}
A classical problem in robust statistics is \textit{robust mean estimation}.
It is formulated as follows: 
For a known family of distributions \(\cP\), a learner is provided with a dataset of \(n\) points, where one part comes from a distribution \(P \in \mathcal{P}\), 
while the rest is generated by an adversarial model. 
The task of the learner is to reliably and accurately estimate \(\E_{P} X\), the mean of \(P\).

A few clarifications are in order. 
First, there exist several different adversarial models, some of them listed below.
In what follows, \(\varepsilon \in (0, 1/2)\) denotes the proportion of adversarial points and \(n\) denotes the total dataset size.
\begin{enumerate}
    \item \textit{Strong adversary.} In this model, in the beginning \(n\) samples are generated i.i.d. from \(P\). 
    Adversary removes or replaces up to \(\lceil \varepsilon n \rceil\) of them with arbitrary points.
    The learner then receives the modified dataset.
    \item \textit{Weak adversary.} Here, adversary picks an arbitrary distribution \(Q\).
    The learner receives the dataset of i.i.d. samples from the mixture \((1 - \varepsilon) P + \varepsilon Q\).
    \item \textit{Mean-shift contamination.} For this model, let \(\overline{P}\) denote the centered distribution \(P\).
    The adversary picks \(k \coloneqq \lceil \varepsilon n \rceil\) points \(z_1, \ldots, z_k\).
    The final dataset consists of \(n - k\) i.i.d. samples from \(P\) and the remaining \(k\) points are obtained by sampling \(x_i \overset{\mathrm{i.i.d.}}{\sim} \overline{P}\) and adding \(z_i\) to it.
\end{enumerate}
\noindent
These models are ordered by how restricted the adversary is. Weak adversary model is also known as Huber contamination model.
In all models, adversary has the knowledge of \(\cP\), and of the algorithm which the learner will be using (apart from the internal randomness of the algorithm).

Second, our goal is to design an algorithm for the learner that is computationally efficient in high dimenions.
In particular, its time and sample complexity must be (quasi-)polynomial in the dimension \(d\) of the samples.

Finally, \emph{reliable} estimate means that the guarantees of the algorithm must hold with high probability over the randomness of the sampling procedure and the internal randomness of the algorithm.
\emph{Accurate} estimate means that the error (usually measured as the \(\ell_2\) distance between the estimate and the true parameter)
does not depend on the dimension \(d\).

In one dimensional case, the median is a great candidate for an efficient and robust mean estimator. 
Indeed, when the samples come from \(\cN(\mu, 1)\) in the strong adversary model \dd{how general is this?}, 
it is easy to show that the median \(\muhat\) satisfies \(\norm{\muhat - \mu}_2 = O(\varepsilon)\) with high probability.
From straightforward TV distance arguments, \(O(\varepsilon)\) is the optimal achievable error.

However, computing median in high dimensions is challenging. 
Consider for simplicity the case \(\cP = \set{\cN(\mu, I), \text{ for } \mu \in \R^d}\).
If we compute median coordinate-wise, then the (in general unimprovable) error is \(O(\sqrt{d} \varepsilon)\), 
which is not accurate, since it depends on the dimension \(d\).
There exists another generalization of median to high dimensions, called \textit{Tukey median}, which is defined as follows:
\begin{equation}
    \mu_{\mathrm{Tukey}} \coloneqq \arg \max_{\muhat \in \R^d} \inf_{v \in \R^d} \Pr(v^\top (X - \muhat) \geq 0).
\end{equation}

Tukey median is an accurate estimator: it achieves error \(O(\varepsilon)\) with high probability. 
However, to compute Tukey median is an NP-hard problem, thus it cannot be used as a candidate for an efficient algorithm.

Recent breakthrough results \dd{add citation} show how to achieve error \(O(\varepsilon)\) efficiently.
One of the ideas is a filtering approach.

\subsection{Clustering and list-decodable mean estimation}

\begin{algorithm}[t]
    \caption{K-Means}
    \label{alg:kmeans}
        \begin{algorithmic}[1]
            \Require Samples \(S = \Set{x_1, \ldots, x_{n}}\), number of centers \(k\).
            \Ensure Clusters \(\mathcal{C} = \Set{C_1, \ldots, C_k}\). 
            \State Initialize cluster centers \(\mu_1, \ldots, \mu_k\). \Comment{Can be done at random, or with some heuristic, see K-Means++ \dd{add citation}}
            \State \(\mathcal{C} \gets \emptyset\).
            \State Form clusters \(\mathcal{C}' = \Set{C'_1, \ldots, C'_k}\) by assigning each point \(x_i\) to the closest cluster center \(\mu_t\).
            \While{\(\mathcal{C} \neq \mathcal{C}'\)}
                \State \(\mathcal{C} \gets \mathcal{C}'\).
                \State For each \(t \in [k]\) set \(\mu_t\) as the average of \(C'_t\).
                \State Recompute clusters \(\mathcal{C}' = \Set{C'_1, \ldots, C'_k}\) as before.
            \EndWhile
        \State \Return \(\mathcal{C}\)
        \end{algorithmic}
    \end{algorithm}


Clustering is one of the most commonly known problems in unsupervised learning.
The goal of the clustering is to split a given dataset into several non-intersecting groups.
The desirable condition of this is split is that, for some similarity score or a metric, samples within the same group (or cluster), 
are more similar or closer than samples from different groups.

Among well-known solutions for clustering are K-Means (see~\Cref{alg:kmeans}) and DBSCAN \dd{add citation}.
Both methods are widely used in practice, and are established methods of unsupervised learning.
However, the theoretical guarantees for them are lacking, especially in high dimensions. 

Consider the sampling process where the data is generated from the spherical Gaussian mixture model (GMM):
\begin{equation}
    X_1, \ldots, X_n \overset{\iid}{\sim} \frac{1}{k} \sum_{i=1}^k \cN(\mu_i, I)
\end{equation}

A natural question is: how large does the separation \(\min_{i \neq j} \norm{\mu_i - \mu_j}_2\) needs to be,
to allow for an efficient clustering procedure? 
This is an active area of research in theoretical computer science \dd{add citation}, 
and the state-of-the-art methods are quite distinct from the classical K-Means or DBSCAN.
One of the promising approaches to design accurate and efficient clustering algorithms is 
based on the Sum-of-Squares hierarchy; we refer to \dd{add citation} for detailed survey.

Interestingly, there is a close connection between robust mean estimation and clustering problems.
Recall that in the definition of robust mean estimation, we constrained the proportion of adversarial points to \(\varepsilon < 1/2\).
In \emph{list-decodable mean estimation}, adversarial points constitute instead the majority of points, i.e., proportion of inlier samples \(\alpha \coloneqq 1 - \varepsilon < 1/2\).

Consider the case when the data is sampled from uniform GMM with two mixture components.
Clearly, we need to relax our definition of accurate estimate: it is information-theoretically impossible to distinguish which component is a true one (inlier), and which is generated adversarially.
To circumvent this issue, list-decodable paragidm \dd{add citation} allows the algorithm to output a (short) \emph{list of estimators} 
with the guarantee that with high probability there exists an element in this list close to the true parameter.
In the example above, a possible algorithm can output list of size two, containing centers of both mixture components.

Formally speaking, in list-decodable mean estimation data is sampled from the distribution \(\alpha P + (1 - \alpha) Q\),
where \(P \in \cP\) is the distribution of interest (inlier distribution), and \(Q\) is an adversarial distribution.
Parameter \(\alpha \in (0, 1/2)\) controls the proportion of inlier samples, and the goal as before is to estimate the mean of \(P\).

To see how list-decodable mean estimation is connected to clustering, 
consider our previous example where data is generated from \(\frac{1}{n} \sum_{i=1}^k \cN(\mu_i, I)\).
If we set \(\cP = \Set{\cN(\mu, I) \text{ for } \mu \in \R^d}\), 
we can feed this data to the algorithm which performs list-decodable mean estimation.
Since any \(\cN(\mu_i, I)\) might be the inlier distribution (and the remaining ones will define \(Q\)),
the list-decodable mean estimator will output a single list which contains approximation to \emph{all means} \(\mu_i\).
With some postprocessing \dd{add citation}, this can be used to cluster all samples.
Curiously, such methods can be made robust to small adversarial noise. 
\dd{add more for the transition.}
\subsection{Mixture learning with corruptions}

Existing works on robust mixture learning such as \cite{diakonikolas2018list,bakshi2022robustly}
consider the problem when the fraction of additive adversarial outliers is smaller than the weight of the smallest subgroup, i.e. \(\e < \wmin\).
However, for large outlier proportions where $\e \geq \wmin$, these algorithms are not guaranteed to recover small clusters with  \(w_i \leq \e\). 
In this case, outliers can form additional spurious clusters
that are indistinguishable 
from small inlier groups. As a consequence, generating a list of size equal to the number of components would possibly lead to neglecting the means of small groups.
In order to ensure that the output contains 
a precise estimate for each of the small group means, 
it is thus necessary  
for the estimation algorithm to provide a list whose size is strictly larger than the number of components.
We call this paradigm \emph{list-decodable mixture learning} (LD-ML),
following the footsteps of a long line of work on list-decodable learning (see \Cref{sec:prelims,sec:relatedwork}). 

Specifically, the main challenge in LD-ML is to provide a \emph{short} list that contains good mean estimates for all inlier groups.
We first note that there is a minimum list size the algorithm necessarily has to output to guarantee that all groups are recovered. For example, consider an outlier distribution that includes several copies of the smallest inlier group distribution with means spread out throughout the domain. Since inlier groups are indisntinguishable from spurious outlier ones, the shortest list that includes means of all inlier groups must be of size at least $|L| \geq k + \frac{\epsilon}{\min_i w_i}$. Here, $\frac{\epsilon}{\min_i w_i}$ can be interpreted as the minimal list-size overhead that is necessary due to "caring" about groups with weight smaller than $\epsilon$.
The key question is hence how good the error guarantees of an LD-ML algorithm can be when the list size overhead stays close to $\frac{\epsilon}{\min w_i}$,
while being agnostic to $w_i$ aside from the knowledge of $\wmin$.
Furthermore, we are interested in \textit{computationally efficient} algorithms for LD-ML, especially when dealing with high-dimensional data.

\dd{paragraph below rewrite, connect better to previous}
To the best of our knowledge, the only existing efficient algorithms that are guaranteed to recover inlier groups with weights \(w_i \leq \e\) are \emph{list-decodable mean estimation} (LD-ME) algorithms. 
LD-ME algorithms model the data as a mixture of one inlier and outlier distribution with weights \(\alpha \leq 1/2\) and \(1 - \alpha\) respectively. Provided with the weight parameter \(\alpha\), they output a list that contains an estimate
close to the inlier mean with high probability. However, for the LD-ML setting,
the inlier weights $w_i$ are not known and we would have to use LD-ME algorithms
with $\wmin$ as weight estimates for each group.
This leads to suboptimal error in particular for 
large groups, that hence (somewhat counter intuitively) would have to "pay" for
the explicit constraint to recover small groups.
Furthermore, even if LD-ME were provided with \(w_i\), by design it would treat inlier points from other components also as outliers, unnecessarily inflating the fraction of outliers to \(1 - w_i\)
instead of \(\e\). 
\paragraph{Contributions}
In this paper, we propose an algorithm that (i) correctly estimates the weight of each component only given a lower bound and (ii) does not overestimate proportion of outliers when components are well-separated. 
In particular, we construct a meta-algorithm that uses mean estimation algorithms as base learners that are designed to deal with adversarial corruptions.
This meta-algorithm inherits guarantees from the base learner and any improvement of the latter translates to better results for LD-ML. For example, if the base learner runs in polynomial time, so does our meta-algorithm.
Our approach of using the output of weak base learners to achieve better performance is reminiscent of the \emph{boosting} paradigm that is common in machine learning practice. 

\begin{table}[tp]
\begin{center}
\setlength{\tabcolsep}{2.4pt}
\begin{tabular}{l|l|l|l}
    \toprule
    \textbf{Type of inlier mixture} & \textbf{Best prior work} & \textbf{Ours} & \textbf{Inf.-theor.} \\[-0.5ex]
     & & & \textbf{lower bound} \\ \hline
    Large groups & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(\widetilde O(\e / w_i)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(\widetilde O(\e / w_i)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(\Omega(\e / w_i)\)\end{tabular} \\ 
    Small groups & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(O\left(\sqrt{\log \frac{1}{\wmin}}\right)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(O\left(\sqrt{\log\frac{\e + w_i}{w_i}}\right)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(\Omega\left(\sqrt{\log\frac{\e + w_i}{w_i}}\right)\)\end{tabular} \\ 
    Non-separated groups & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(O\left(\sqrt{\log\frac{1}{\wmin}}\right)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(O\left(\sqrt{\log\frac{1}{w_i}}\right)\)\end{tabular} & \begin{tabular}{@{}c@{}}\rule{0pt}{3ex}\(\Omega\left(\sqrt{\log\frac{1}{w_i}}\right)\)\end{tabular}\\
    \bottomrule
    \end{tabular}

\caption{
\small{For a mixture of Gaussian components \(\cN(\mu_i, I_d)\), we show upper and lower bounds for the \textbf{error of the $i$-component} given a output list $L$ (of the respective algorithm) \(\min_{\muhat \in L} \norm{\muhat - \mu_i}\).
When the error doesn't depend on $i$, all means have the same error guarantee irrespective of their weight. 
Note that depending on the type of inlier mixture, different methods in~\cite{diakonikolas2018list} are used as the 'best prior work': robust mixture learning for the first row and list-decodable mean estimation for the rest. 
'Large groups' means that \(\forall j: \: \e \leq w_j\), 'small groups' means \(\exists j: \: \e \geq w_j\). 
In both cases, mixture components are assumed to be separated. For lower bounds, see~\cite{diakonikolas2023algorithmic, regev2017learning}, and~Prop.~\ref{lemma:it_lb_ours}
}
}
\label{tab:method_comparison_two}
\end{center}
\end{table}

Our algorithm achieves significant improvements in error and list-size guarantees for multiple settings. For ease of comparison, we summarize error improvements for inlier Gaussian mixtures in~\Cref{tab:method_comparison_two}. 
The main focus of our contributions is represented in the second row; that is the  setting where outliers outnumber some inlier groups with weight $w_j \leq \epsilon$
and the inlier components are \emph{well-separated}, i.e., \(\norm{\mu_i - \mu_j} \gtrsim\)\footnote{\dd{remove?} We adopt the following standard notation: \(f \lesssim g\), \(f = O(g)\), and \(g = \Omega(f)\) mean that \(f \leq Cg\) for some universal constant \(C > 0\). \(\widetilde O\)-notation hides polylogarithmic terms.} \(\sqrt{\log \frac{1}{\wmin}}\), where \(\mu_i\)'s are the inlier component means.
As we mentioned before \dd{is it mentioned before?}, robust mixture learning algorithms, such as~\cite{bakshi2022robustly, ivkov2022list}, are not applicable here and the best error guarantees in prior work is achieved by an LD-ME algorithm, e.g. from~\cite{diakonikolas2018list}. While its error bounds are of order \(O(\sqrt{\log \frac{1}{\wmin}})\) for a list size of \(O(\frac{1}{\wmin})\), 
our approach guarantees error \(O(\sqrt{\log \frac{\e}{w_i}})\)
for a list size of \(k + O(\frac{\e}{\wmin})\).
Remarkably, we obtain the same error guarantees as if an oracle would run LD-ME on each inlier group \emph{with the correct weight \(w_i\)} separately (with outliers).
Hence, the only cost for recovering small groups is the increased list-size overhead of order \(O(\frac{\e}{\wmin})\).
Further, a sub-routine in our meta-algorithm also obtains novel guarantees under \emph{no} separation assumption,
as shown in the third row of~\Cref{tab:method_comparison_two}.
This algorithm achieves the same error guarantees for similar list size as a base learner that knows 
the correct weights of the inlier components.

Based on a reduction argument from LD-ME to LD-ML, we also provide information-theoretic (IT) lower bounds for LD-ML. If the LD-ME base learners achieve the IT lower bound (possible for inlier Gaussian mixtures), so does our LD-ML algorithm. 
In synthetic experiments, we implement our meta-algorithm with the LD-ME base learner from~\cite{diakonikolas2022clustering} and show clear improvements compared to the only prior method with guarantees, while being comparable or better than popular clustering methods such as k-means and DBSCAN for various attack models.


% Estimating the mean of a distribution from empirical data is one of the most fundamental problems in statistics.
% The mean often serves as the primary summary statistic of the dataset or is the ultimate %of a 
% quantity of interest that is often not precisely measurable.
% In practical applications, data frequently originates from a mixture of multiple groups (also called subpopulations) and a natural goal is to estimate the distinct means of each group separately. 
% For example, we might like to use representative individuals to study how a complex decision or procedure would impact different subpopulations.
% In other applications, such as genetics~\cite{bickel2003robust} or astronomy~\cite{feigelson2012statistical} research, finding the means themselves
% can be a crucial first step towards scientific discovery.
% In both scenarios, the algorithm should output a list of estimates that are close to the unobservable true means. 


