Consider a supervised learning task with training data $(\mathsf x_{i},\mathsf y_{i})_{i\in[n]}$. In this manuscript, we are interested in studying the statistics of linear predictors $f_{\w}(\x)	= \frac{1}{\sqrt{\p}}\w^{\top}\f(\x)$ for a class of fixed feature maps $\f:\R^{\di}\to\R^{\p}$ and weights $\w\in\R^{\p}$ trained via empirical risk minimization:
\begin{align}
    \label{eq:def:erm}
    \hat{\w}_{\lambda} = \underset{\w\in\R^{\p}}{\min}~\sum\limits_{i\in[\n]}\left(\y_{i}-f_{\w}(\x_{i})\right)^{2} + \lambda||\w||^{2}.
\end{align}
Of particular interest is the generalization error:
\begin{align}\label{eq:gen err def}
    \mathcal{E}_{\gen}(\hat{\w}_{\lambda}) = \E\left(\y - f_{\hat{\w}_{\lambda}}(\x)\right)^{2}
\end{align}
where the expectation is over a fresh sample from the same distribution as the training data. More precisely, our results will hold under the following assumptions.

\begin{assumption}[Labels]\label{ass:labels}
    We assume that the labels $y_i$ are generated by another feature map $\fast\colon\R^d\to\R^k$ as
    \begin{equation}
        \y_i = \frac{1}{\sqrt{k}}\theta_\ast^\top \fast(\x_i) + \noise_i,
    \end{equation}
    where $\noise\in\R^n$ is an additive noise vector (independent of the covariates $\x_i$) of zero mean and covariance $\Sigma:=\E \noise\noise^\top$, and $\theta_\ast\in\R^{k}$ is a deterministic weight vector.
\end{assumption}
\begin{assumption}[Data \& Features]\label{ass:data+features}
    We assume that the covariates $\x_i$ are independent and come from a distribution such that
    \begin{condenum}
        \item\label{cond centered} the feature maps $\f,\fast$ are centered\footnote{This is a commonly used assumption which simplifies the analysis. Our techniques also apply to the case of non-zero mean, however doing so would add a rank-one component to the sample covariance matrix, considerably complicating the final expressions for the deterministic equivalents.} in the sense $\E\f(\x_i)=0$, $\E\fast(\x_i)=0$,
        \item\label{cond cov} the feature covariances
        \begin{equation}
            \begin{split}
                \Omega&:=\E \f(\x_i)\f(\x_i)^\top\in\R^{p\times p},\\
                \Psi&:=\E \fast(\x_i)\fast(\x_i)^\top\in\R^{k\times k},\\
                \Phi&:=\E \f(\x_i)\fast(\x_i)^\top\in\R^{p\times k},
            \end{split}
        \end{equation}
        have uniformly bounded spectral norm.
        \item\label{cond Lipschitz conc} scalar Lipschitz functions of the feature matrices
        \begin{equation}
            \begin{split}
                X&:=(\f(\x_1),\ldots,\f(\x_n))\in\R^{p\times n}\\
                Z&:=(\fast(\x_1),\ldots,\fast(\x_n))\in\R^{k\times n}
            \end{split}
        \end{equation} are uniformly sub-Gaussian.
    \end{condenum}
\end{assumption}
\begin{assumption}[Proportional regime]\label{ass:dimensions}
    The number of samples $n$ and the feature dimensions $p,k$ are all large and comparable, see~\Cref{thm genRMT informal} later.
\end{assumption}
\begin{remark}\label{remark suff cond}
    We formulated~\Cref{ass:data+features} as a joint assumption on the covariates distribution and the feature maps. A conceptually simpler but less general condition would be to assume that
    \begin{condenum}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item[(ii')] the covariates $\x_i$ are Gaussian with bounded covariance $\Omega_0:=\E\x_i\x_i^\top$
        \item[(iii')] the feature maps $\f,\fast$ are Lipschitz-continuous
    \end{condenum}
    instead of~\Cref{cond cov,cond Lipschitz conc}.
\end{remark}
The setting above defines a quite broad class of problems, and the results that follow in Section~\ref{sec gen err} will hold under these generic assumptions. The main class of feature maps we are interested in are \emph{deep structured feature models}.
\begin{definition}[Deep structured feature model]\label{deep feature model}
    For any fixed $L\in\N$ and dimensions $d,p_1,\ldots,p_L=p$, let $\f_1,\ldots,\f_L\colon\R\to\R$ be Lipschitz-continuous \emph{activation functions} $\abs{\f_l(a)-\f_l(b)}\lesssim\abs{a-b}$ applied entrywise, and let $W_1\in\R^{p_1\times d}, W_2\in\R^{p_2\times p_1},\ldots$ be deterministic \emph{weight matrices} with uniformly bounded spectral norms, $\norm{W_l}\lesssim 1$. We then call
\begin{align}\label{deep feature eq}
    \f(\x) := \f_{L}\left(\W_{L}\f_{L-1}\left(\cdots W_{2}\f_{1}\left(\W_{1}\x\right)\right)\right).
\end{align}%\vspace{-0.364cm}
a \emph{deep structured feature} model. 
\end{definition}
Note that~\cref{deep feature eq} defines a Lipschitz-continuous map\footnote{$\norm{\f(W\x)-\f(W\x')}^2 = \sum_{i} \abs{\f(w_i^\top \x)-\f(w_i^\top \x')}^2 \lesssim \sum_i \abs{w_i^\top(\x-\x')}^2 = \norm{W(\x-\x')}^2\lesssim \norm{\x-\x'}^2$} $\f\colon\R^d\to\R^{p},\fast\colon\R^d\to\R^k$ and therefore if both $\f,\fast$ are deep structured feature models (with distinct parameters in general), then~\Cref{ass:data+features} is satisfied whenever the feature maps $\f,\fast$ are centered\footnote{It is sufficent that e.g.\ $\phi_l$ is odd, and $\x_i$ is centered.} with respect to Gaussian covariates $\x_i$. As hinted in the introduction we will be particularly interested in one sub-class of~\Cref{deep feature model} known as \emph{Gaussian rainbow networks}.
\begin{definition}[Gaussian rainbow ensemble] 
\label{def:rainbow}
Borrowing the terminology of \cite{guth2023rainbow}, we define a fully-connected, $\depth$-layer \emph{Gaussian rainbow network} as a random variant of~\Cref{deep feature model} where for each $\ell$ the hidden-layer weights $\W_{\ell} = Z_{\ell}C_{\ell}^{1/2}$ are random matrices with $Z_{\ell}\in\R^{\p_{\ell+1}\times \p_{\ell}}$ having zero mean and i.i.d.\ variance $\frac{1}{p_\ell}$ Gaussian entries and $C_{\ell}\in\R^{p_{\ell}\times p_{\ell}}$ being uniformly bounded covariance matrices, which we allow to depend on previous layer weights $Z_1,\ldots,Z_{l-1}$. 
\end{definition}
Note that Gaussian rainbow networks above can be seen as a generalization of the deep random features model studied in \cite{schroder2023deterministic, bosch2023precise, Fan2020SpectraOT}, with the crucial difference that the weights are structured. 

\subsection*{Notation}
For square matrices $A\in\R^{n\times n}$ we denote the averaged trace by $\braket{A}:=n^{-1}\Tr A$, and for rectangular matrices $A\in\R^{n\times m}$ we denote the Frobenius norm by $\norm{A}_F^2:=\sum_{ij}\abs{a_{ij}}^2$, and the operator norm by $\norm{A}$. For families of non-negative random variables $X(n),Y(n)$ we say that $X$ is \emph{stochastically dominated} by $Y$, and write $X\prec Y$, if for all $\epsilon,D$ it holds that $P(X(n)\ge n^{\epsilon} Y(n))\le n^{-D}$ for $n$ sufficiently large. For a centered random vector \(x \in \R^d\) we denote its \emph{sub-Gaussian norm} as
        \(\norm{x}_{\psi_2} \coloneqq \inf_{\sigma 
        \geq 0} \{\E \exp^{\braket{v, x}} \leq \exp^{\frac{\norm{v}^2 \sigma^2}{2}}\ \forall\, v \in \R^d\}\).