Deep neural networks are the backbone of most successful machine learning algorithms in the past decade. Despite their ubiquity, a firm theoretical understanding of the very basic mechanism behind their capacity to adapt to different types of data and generalise across different tasks remains, to a large extent, elusive. For instance, what is the relationship between the inductive bias introduced by the network architecture and the representations learned from the data, and how does it correlate with generalisation? Despite the lack of a complete picture, insights can be found in recent empirical and theoretical works.

On the theoretical side, a substantial fraction of the literature has focused on the study of deep networks at initialisation, motivated by the lazy training regime of large-width networks with standard scaling. Besides the mathematical convenience, the study of random networks at initialisation have proven to be a valuable theoretical testbed -- allowing in particular to capture some empirically observed behaviour, such as the double-decent \cite{belkin2019reconciling} and benign overfitting \cite{Bartlett20} phenomena. As such, proxys for networks at initialisation, such as the Random Features (RF) model \cite{Rahimi2007RandomFF} have thus been the object of considerable theoretical attention, with their learning being asymptotically characterized in the two-layer case \cite{Goldt2021TheGE, Goldt2020ModellingTI, Gerace2020GeneralisationEI, Hu2020UniversalityLF, Dhifallah2020, Mei2019TheGE, Mei2021GeneralizationEO} and the deep case \cite{ZavatoneVeth2022ContrastingRA, schroder2023deterministic, bosch2023precise, zavatone2023learning}. With the exception of \cite{Gerace2020GeneralisationEI, mel2022anisotropic} (limited to two-layer networks) and \cite{zavatone2023learning} (limited to linear networks), all the analyses for non-linear deep RFs assume unstructured random weights. In sharp contrast, the weights of trained neural networks are fundamentally structured - restricting the scope of these results to networks at initialization.

Indeed, an active research direction consists of empirically investigating how the statistics of the weights in trained neural networks encode the learned information, and how this translates to properties of the predictor, such as inductive biases \cite{Thamm_2022, JMLR:v22:20-410}. Of particular relevance to our work is a recent observation by \cite{guth2023rainbow} that a random (but structured) network with the weights sampled from an ensemble with matching statistics can retain a comparable performance to the original trained neural networks. In particular, for some tasks it was shown that second order statistics suffices -- defining a Gaussian \emph{rainbow network} ensemble.

Our goal in this manuscript is to provide an exact asymptotic characterization of the properties of \emph{Gaussian rainbow networks}, i.e. deep, non-linear networks with structured random weights. Our \textbf{main contributions} are:
\begin{itemize}
    \item We derive a tight asymptotic characterization of the test error achieved by performing ridge regression with Lipschitz-continuous feature maps, in the high-dimensional limit where the dimension of the features and the number of samples grow at proportional rate. This class of feature maps encompasses as a particular case Gaussian rainbow network features.
    \item The asymptotic characterization is formulated in terms of the population covariance of the features. For Gaussian rainbow networks, we explicit a closed-form expression of this covariance, formulated as in the unstructured case \cite{schroder2023deterministic} as a simple linear recursion depending on the weight matrices of each layer. These formulae extend similar results of \cite{Cui2023, schroder2023deterministic} for independent and unstructured weights to the case of structured --and potentially correlated-- weights.
    \item We empirically find that our theoretical characterization captures well the learning curves of some networks trained by gradient descent in the lazy regime.
\end{itemize}
\paragraph{Code --} The code for the numerical experiments described in~\Cref{app: numerics} is openly available in  \href{https://github.com/wirhabenzeit/feature-ridge-regression}{this repository.}

\subsection*{Related works}
\paragraph{Random features} (Rfs) were introduced in \cite{Rahimi2007RandomFF} as a computationally efficient way of approximating large kernel matrices. In the shallow case, the asymptotic spectral density of the conjugate kernel was derived in \cite{Liao2018, Pennington2019NonlinearRM, Benigni2021}. The test error was on the other hand characterized in \cite{Mei2019TheGE, Mei2021GeneralizationEO} for ridge regression, and extended to generic convex losses by \cite{Gerace2020GeneralisationEI, Goldt2021TheGE, Dhifallah2020}, and in \cite{Sur2020, Loureiro2021CapturingTL, Bosch2022} for other penalties. RFs have been studied as a model for networks in the lazy regime, see e.g. \cite{Ghorbani2019, Ghorbani2020WhenDN, NEURIPS2019_5481b2f3, pmlr-v139-refinetti21b}. The role of structure in the RF weights was discussed in \cite{Gerace2022} for rotationally invariant weights and \cite{mel2022anisotropic, pandey2022structured} for anisotropic Gaussian weights.
\paragraph{Deep RFs -- } Recent work have addressed the problem of extending these results to deeper architectures. In the case of linear networks, a sharp characterization of the test error is provided in \cite{ZavatoneVeth2022ContrastingRA} for the case of unstructured weights and \cite{zavatone2023learning} in the case of structured weights. For non-linear RFs, \cite{schroder2023deterministic} provides deterministic equivalents for the sample covariance matrices, and \cite{schroder2023deterministic, bosch2023precise} provide a tight characterization of the test error. The recent work of \cite{guth2023rainbow} provides empirical evidence that for a given trained neural network, a resampled network from an ensemble with matching statistics (\emph{rainbow networks}) might achieve comparable generalization performance, thereby partly bridging the gap between random networks and trained networks.
