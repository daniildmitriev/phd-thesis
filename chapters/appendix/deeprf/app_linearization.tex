\subsection{Technical background}
\label{app:technical}
In this section we state several definition and propositions from~\cite{nourdin2012normal}, that will be used further in our arguments.
%\begin{definition}
%    Let \(\gamma\) define a standard Gaussian probability measure, i.e. \(\gamma(A) = %\frac{1}{\sqrt{2\pi}}\int_A \exp(-x^2/2) \mathrm{d}x\).
%\end{definition}
%\begin{definition}
%    Let \(\cS\) denote the set of \(C^{\infty}\)-functions, 
%    such that \(f\) and all its derivatives have at most polynomial growth.
%\end{definition}
%\begin{definition}[\cite{nourdin2012normal}, Definition 1.2.1]
%    Let \(\operatorname{Dom}\delta^p \subset L^2(\gamma)\) be a set of functions \%(g\), such that there exists \(c > 0\) satisfying that for all \(f \in \cS\),
    %\begin{equation}
     %   \abs{\int_{\R} f^{(p)}(x) g(x)\mathrm{d}\gamma(x)} \leq c \sqrt{\int_{\R}f^2(x)\mathrm{d}\gamma(x)},
    %\end{equation}
%\end{definition}
%\begin{definition}[\cite{nourdin2012normal}, Definition 1.2.2]
 %   Let \(p \geq 1\) be an integer. The \(p\)th divergence operator \(\delta^p\) is defined as follows.
  %  If \(g \in \operatorname{Dom}\delta^p\), then \(\delta^p g\) is the unique element of \(L^2(\gamma)\) characterized by the duality formula: for all \(f \in \cS\),
   % \begin{equation}
        %\int_{\R}f^{(p)}(x)g(x)\mathrm{d}\gamma(x) = \int_{\R}f(x)\delta^pg(x) %\mathrm{d}\gamma(x).
    %\end{equation}
    %We denote \(\delta \equiv \delta^1\).
%\end{definition}
%\begin{definition}[\cite{nourdin2012normal}, Definition 1.4.1]
 %   Let \(p \geq 0\) be an integer.
  %  We define the \(p\)th Hermite polynomial as \(H_0 = 1\), \(H_p = \delta^p 1\) for \(p \geq 1\).
   % First Hermite polynomials are as follows: \(H_1(x) = x\), \(H_2(x) = x^2 - 1\), \(H_3(x) = x^3 - 3x\), etc.
%\end{definition}
%\begin{proposition}[\cite{nourdin2012normal}, Proposition 2.2.1]
%    Let \(Z, Y \sim \cN(0, 1)\) be jointly Gaussian. Then, for all integers \(n, m \geq 0\):
%\begin{equation}
%    \E H_n(Z) H_m(Y) = \delta_{nm} n! Cov(Z, Y)^n,
%\end{equation}
 %   where \(\delta_{nm} = \mathbb{I}(n = m)\).
%\end{proposition}
Let \(x \in \R^d\) be a mean-zero Gaussian vector with covariance \(\E x x^T = I\).
Let \(X = \{X(v) \coloneqq v^{\top} x, \text{ for } v \in \R^d\}\) be a collection of jointly Gaussian centered random variables.
Note that \(\E X(g) X(h) = g^{\top} h\).
The theory of \textit{Wiener chaos}, which will be introduced shortly, 
can be used to study functions on the probability space \((\Omega, \mathcal{F}, P)\), 
where \(\mathcal{F}\) is generated by \(X\). For our needs, we only state the results for the explicit construction of \(X\), however, note that the results from~\cite{nourdin2012normal} are about general separable Hilbert spaces.

Following (\cite{nourdin2012normal}, Definition 2.2.3),
we write \(\cH_n\) to denote the closed linear subspace of \(L^2(\Omega, \mathcal{F}, P)\) 
generated by the random variables of type \(H_n(X(h)), h \in \R^d\), \(\norm{h} = 1\), where \(H_n\) is the \(n\)-th \emph{Hermite polynomial}. We call \(\cH_n\), the \(n\)-th Wiener chaos.

% \begin{proposition}[\cite{nourdin2012normal}, Theorem 2.2.4 + Example 2.2.6]
%     Every random variable \(F \in L^2(\Omega, \mathcal{F}, P)\) admits a unique expansion of the type
%     \(F = \E F + \sum_{n = 1}^{\infty} F_n\), where \(F_n \in \cH_n\), 
%     and the series converges in \(L^2(\Omega, \mathcal{F}, P)\).

%     Furthermore, if \(F = \phi(X(h)) = \phi(h^{\top} x)\), for some \(h \in R^{d}\), \(\norm{h} = 1\),
%     and \(\phi: \R \to \R\) is a Borel function, s.t. \(\E \phi^2(N) < \infty\) for \(N \sim \cN(0, 1)\),
%     then 
%     \begin{equation}
%         F = \sum_{p = 0}^{\infty} \frac{\E \phi^{(p)}( \sqrt{h^{\top} h}N)}{p!}  H_p(X(h)).
%     \end{equation}
% \end{proposition}

\begin{definition}
    Let \(L^2(\Omega, \mathfrak{H}^{\tilde \otimes p})\) be the space of functions \(f: \R^{d \times p} \to \R\), such that \(f\) is square-integrable and
    \begin{equation}
        f(a_1, \ldots, a_p) = \frac{1}{p!} \sum_{\sigma \in S_p} f(a_{\sigma(1)}, \ldots, a_{\sigma(p)}).
    \end{equation}
\end{definition}
Let \(\sS\) denote the set of all random variables of the form \(f(X(h_1), \ldots, X(h_m))\),
where \(f: \R^{m} \to \R\) is a \(C^{\infty}\)-function. 
\begin{definition}[\cite{nourdin2012normal}, Definition 2.3.2]
    Let \(F \in \sS\) and \(p \geq 1\) be an integer. The \(p\)th Malliavin derivative of \(F\) (with respect to \(X\))
    is the element of \(L^2(\Omega, \mathfrak{H}^{\tilde \otimes p})\), defined by
    \begin{equation}
        D^p F \coloneqq \sum_{i_1, \ldots, i_p = 1}^{m} \frac{\partial^p f}{\partial x_{i_1} \ldots \partial x_{i_p}} (X(h_1), \ldots, X(h_m)) h_{i_1} \otimes \ldots \otimes h_{i_p}.
    \end{equation}
\end{definition}
\begin{proposition}[\cite{nourdin2012normal}, Proposition 2.3.7]
    Let \(\phi: \R^m \to \R\) be a continuously differentiable function with bounded partial derivatives.
    Suppose that \(F = (F_1, \ldots, F_m)\) is a random vector whose components are functions with derivatives in \(L^q(\gamma)\), for some \(q \geq 1\).
    Then, derivative of \(\phi(F)\) also lies in \(L^q(\gamma)\) and
    \begin{equation}
        D \phi(F) = \sum_{i = 1}^m \frac{\partial \phi}{\partial x_i}(F) D F_i.
    \end{equation}
\end{proposition}
\begin{definition}[\cite{nourdin2012normal}, Definition 2.5.2]
We define $\delta^p u$ as the unique element of $L^2$ satisfying 
\[\E[F\delta^p(u)]= E[\braket{D^p F,u}_{\mathfrak H^{\otimes p}}].\]
\end{definition}
\begin{definition}[\cite{nourdin2012normal}, Definition 2.7.1]
    Let \(p \geq 1\) and \(f \in \mathfrak{H}^{\tilde \otimes p}\). The \(p\)th multiple integral of \(f\) with respect to \(X\) is defined by
    \(I_p(f) = \delta^p(f)\).
\end{definition}
\begin{proposition}[\cite{nourdin2012normal}, Proposition 2.7.5]
    Fix integers \(1 \leq q \leq p\) and \(f \in \mathfrak{H}^{\tilde \otimes p}\) and \(g \in \mathfrak{H}^{\tilde \otimes q}\).
    We have
    \begin{equation}
        \E I_p(f) I_q(g) = \delta_{pq} p! \braket{f, g}_{\mathfrak{H}^{\otimes p}}
    \end{equation}
\end{proposition}
\begin{theorem}[\cite{nourdin2012normal}, Theorem 2.7.7]
    Let \(f \in \mathfrak{H}\) be such that \(\norm{f}_{\mathfrak{H}} = 1\). Then, for any integer \(p \geq 1\), we have
    \begin{equation}
        H_p(X(f)) = I_p(f^{\otimes p}),
    \end{equation}
    where $H_p$ is the $p$-th Hermite polynomial. 
\end{theorem}
\begin{corollary}[\cite{nourdin2012normal}, Corollary 2.7.8]
\label{cor:wiener_chaos_general}
    Every \(F \in L^2(\Omega)\) can be expanded as 
    \begin{equation}
        F = \E F + \sum_{p = 1}^{\infty} I_p(f_p),
    \end{equation}
    for some unique collection of kernels \(f_p \in \mathfrak{H}^{\tilde \otimes p}\), \(p \geq 1\).
    Moreover, if \(F \in C^{\infty}\), then for all \(p \geq 1\), 
    \begin{equation}
        f_p = \frac{1}{p!}\E D^{p} F.
    \end{equation}
\end{corollary}
\begin{theorem}[\cite{nourdin2012normal}, Theorem 5.1.5]
\label{thm:clt}
    Let \(F \in C^{\infty}\) be a square-integrable function. Let \(\E F = 0\) and \(\E F^2 = \sigma^2 > 0\) and \(N \sim \cN(0, \sigma^2)\).
    Let \(h: \R \to \R\) be \(C^2\) with \(\norm{h''}_{\infty} < \infty\).
    Then,
    \begin{equation}
        \abs{\E h(N) - \E h(F)} \leq \frac{1}{2} \norm{h''}_{\infty} \E \left[ \abs{\braket{DF, -D L^{-1} F} - \sigma^2}\right].
    \end{equation}
\end{theorem}
Finally, we use the following multivariate version of the previous theorem.
\begin{theorem}[\cite{nourdin2012normal}, Theorem 6.1.2]
\label{thm:multi-clt}
    Fix \(c \geq 2\), and let \(F = (F_1, \ldots, F_c)\) be a random vector such that \(F_i \in \mathbb{D}^{1, 4}\)
    with \(\E F_i = 0\) for any \(i\).
    Let \(C \in \mathcal{M}_c(\R)\) be a symmetric non-negative definite matrix,
    and let \(N \sim \cN(0, C)\).
    Then, for any \(h\: : \: \R^c \to \R\) belonging to \(\mathcal{C}^2\)
    such that \(\norm{h''}_{\infty} < \infty\),
    \begin{equation}
        \abs{\E h(F) - \E h(N)} \leq \frac{c}{2} \norm{h''}_{\infty} \sqrt{\sum_{i,j=1}^c \E \left[\left(C_{ij} - \braket{D F_j, -DL^{-1}F_i}_{\mathfrak{H}}\right)^2\right]}
    \end{equation}
\end{theorem}
\begin{remark}
    We believe there is a mistake in the original formulation of Theorem 6.1.2 in~\cite{nourdin2012normal}.
    In particular, originally the expression on the right hand side did not contain \(c\) term.
\end{remark}
For our application, we need the following expansion: for smooth odd functions \(f\), and matrix \(W \in \R^{k \times d}\), we can write
\begin{equation}
\label{eq:wiener_chaos_expansion}
\begin{aligned}
    f(Wx)_i = f(w_i^{\top} x) = \sum_{p \geq 1} \frac{\E f^{(p)} ((W W^{\top})^{1/2}_{ii} N)}{p!} I_p(w_i^{\otimes p}), 
\end{aligned}
\end{equation}
where \(w_i \in \R^d\) is the \(i\)-th row of \(W\). Here without loss of generality we assume that $x$ has i.i.d.\ entries, the general case of covariance $\Omega_0$ then follows upon redefining $W_1\mapsto W_1\sqrt{\Omega_0}$. 
% Finally, we restate the following results:
% \begin{lemma}[Hanson-Wright inequality,~\cite{rudelson2013hanson}]
%     For random vectors $x$ with independent entries satisfying $\E x_i=0$ and $\norm{x_i}_{\psi_2}\le K$ it holds that 
%     \begin{equation}
%         \mathbf P\Bigl( \abs{x^\top A x- \E x^\top A x}\ge t \Bigr)\le 2 \exp\Bigl(-c\min\Bigl[\frac{t^2}{K^4\norm{A}_F^2},\frac{t}{K^2\norm{A}}\Bigr]\Bigr)
%     \end{equation}
%         which, in turn, implies
%     \begin{equation}
%         \abs{x^\top A x- \E x^\top A x} \prec K^2 \norm{A}_F,
%     \end{equation}
%     for all $n\times n$ matrices $A$, for some universal constant $c$.
% \end{lemma}
% \begin{corollary}
% \label{cor:hanson_wright}
%     For random vectors $x,y$ with entries satisfying $\norm{x_i}_{\psi_2}+\norm{y_i}_{\psi_2}\le K$ which are such that $x_i,y_i$ are independent of $x_j,y_j$ for all $i\ne j$ ($x_i,y_i$ may be dependent), it holds that 
%     \begin{equation}
%         \abs{x^\top A y-\E x^\top A y}\prec (K+\norm{\E x}+\norm{\E y})K \norm{A}_F
%     \end{equation}
%     for all \emph{symmetric} $n\times n$ matrices $A$. 
% \end{corollary}
Let \((f_{\ell}): \R \to \R\) be a sequence of smooth functions, 
    \((W^{\ell})\) be a sequence of matrices.
    We define a sequence of vectors \(x^i\), such that \(x^0 \coloneqq x \sim \cN(0, I)\), 
    \(x^{\ell + 1} = f_{\ell + 1}(W^{\ell + 1} x^{\ell}).\)
% \begin{lemma}[\dd{remove if not resolved with \(\wt \bigotimes\)}]
% \label{lem:der_recursion}
%     Let \((f_{\ell}): \R \to \R\) be a sequence of smooth functions, 
%     \((W^{\ell})\) be a sequence of matrices.
%     We define a sequence of vectors \(x^i\), such that \(x^0 \coloneqq x \sim \cN(0, I)\), 
%     \(x^{\ell + 1} = f_{\ell + 1}(W^{\ell + 1} x^{\ell}).\)
%     Then, \(D^p x^0_j = \mathbb{I}(p = 1)e_j\) and
%     \begin{equation}
%         D^p x^{\ell}_j = \sum_{\pi \vdash [p]} f_{\ell}^{\abs{\pi}}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\bigotimes_{B \in \pi} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right).
%     \end{equation}
% \end{lemma}
% \begin{proof}
%     Using Faa di Bruno formula, we arrive at \dd{add here details}
%     \begin{equation}
%         D^p x^{\ell}_j = \sum_{\pi \vdash [p]} f_{\ell}^{\abs{\pi}}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\widetilde\bigotimes_{B \in \pi} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right),
%     \end{equation}
%     where \(\widetilde{\bigotimes}\) is a symmetrized tensor product. We can write
%     \begin{equation}
%         \widetilde\bigotimes_{B \in \pi} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right)
%         = \frac{1}{p!} \sum_{\sigma \in S_p} \bigotimes_{B \in \pi \circ \sigma} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right),
%     \end{equation}
%     which implies that
%     \begin{equation}
%         D^p x^{\ell}_j = \frac{1}{p!} \sum_{\sigma \in S_p} \sum_{\pi \vdash [p]} f_{\ell}^{\abs{\pi}}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\bigotimes_{B \in \pi \circ \sigma} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right).
%     \end{equation}
%     By renaming \(\pi' = \pi \circ \sigma\) and noting that summands of the first sum do not depend on \(\sigma\), we arrive at the conclusion:
%     \begin{equation}
%     \begin{aligned}
%         D^p x^{\ell}_j &= \frac{1}{p!} \sum_{\sigma \in S_p} \sum_{\pi' \vdash [p]} f_{\ell}^{\abs{\pi'}}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\bigotimes_{B \in \pi'} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right) \\
%         &= 
%         \sum_{\pi \vdash [p]} f_{\ell}^{\abs{\pi}}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\bigotimes_{B \in \pi} \left(\sum_{j'} W_{jj'}^{\ell} D^{\abs{B}} x_{j'}^{\ell - 1}\right).
%     \end{aligned}
%     \end{equation}
% \end{proof}
% Let \(\cD^p\) be defined as follows:
% \(\cD^p x^0_j = \mathbb{I}(p = 1) e_j\) 
% and 
% \begin{equation}
% \cD^p x^{\ell}_j = \sum_{\pi \vdash [p]} \mathrm{f}_{\ell}^{\abs{\pi}}\bigotimes_{B \in \pi} \left(\sum_{j'} W_{jj'}^{\ell} \cD^{\abs{B}} x_{j'}^{\ell - 1}\right),
% \end{equation}
% where \(\mathrm{f}^{\abs{\pi}}_{\ell} \coloneqq \E f^{(\abs{\pi})}_{\ell}\left(\sum_{j'} W_{jj'}^{\ell} x_{j'}^{\ell - 1}
%         \right)\).
% \begin{lemma}[\dd{to prove or to remove}]
%      We have that 
%      \begin{equation}
%          \braket{\E D^p x^{\ell}_i, \E D^p \tilde x^{\ell}_j} = \braket{\cD^p x^{\ell}_i, \cD^p \tilde x^{\ell}_j} + O(d^{-1/2}).
%      \end{equation}
% \end{lemma}
\begin{lemma}[Weak correlation]
\label{lemma:weak_corr}
Let \(b \geq 1\) be a fixed integer. Let \(h_0, h_1, \ldots, h_b\) be a collection of functions. Then, we have that
    \begin{equation}
        \begin{aligned}
        &\E \left[ h_0(u^{\top} f_1(W^1 x)) \prod_{i=1}^b h_i(w_{i}^{\top} x)\right] \\
        &\quad =\E h_0(u^{\top} f_1(W^1 x)) \prod_{i=1}^b \E h_i(w_{i}^{\top} x) + O(d^{-1/2}).
        \end{aligned}
    \end{equation}
\end{lemma}
\begin{proof}
    The fact that \(u_i \lesssim d^{-1/2}\) and \(f_1(w^{\top} x) \lesssim 1\) together with perturbation analysis imply that
    \begin{equation}
        \begin{aligned}
        &\E \left[ h_0(u^{\top} f_1(W^1 x)) \prod_{i=1}^b h_i(w_{i}^{\top} x)\right] \\
        &\quad = 
        \E \left[ h_0\left(\sum_{k\geq b + 1} u_k f_1(w_k^\top x)\right) \prod_{i=1}^b h_i(w_{i}^{\top} x)\right] + O(d^{-1/2}).
        \end{aligned}
    \end{equation}
    Let \(A \coloneqq h_0\left(\sum_{k\geq b + 1} u_k f_1(w_k^\top x)\right)\) and \(B \coloneqq \prod_{i=1}^b h_i(w_{i}^{\top} x)\). Note that for any \(p \geq 1\), \(\braket{\E D^p A, \E D^p B}\) constitutes of products of \(\braket{w_i, w_j}\), where \(i \neq j\). Each of these products is of order \(O(d^{-1/2})\) by our assumptions. Therefore, in total, \(\braket{\E D^p A, \E D^p B} = O(d^{-p/2})\). This implies that 
    \begin{equation}
        \begin{aligned}
        &\E \left[ h_0\left(\sum_{k\geq b + 1} u_k f_1(w_k^\top x)\right) \prod_{i=1}^b h_i(w_{i}^{\top} x)\right]\\
        &\quad = \E \left[ h_0\left(\sum_{k\geq b + 1} u_k f_1(w_k^\top x)\right)\right] \E \left[ \prod_{i=1}^b h_i(w_{i}^{\top} x)\right] + O(d^{-1/2}).
        \end{aligned}
    \end{equation}
    Similarly, it follows that \(\E \prod_{i=1}^b h_i(w_{i}^{\top} x) = \prod_{i=1}^b \E h_i(w_{i}^{\top} x) + O(d^{-1/2})\) and finally, using perturbation analysis again, we conclude that
    \begin{equation}
        \begin{aligned}
        &\E \left[ h_0(u^{\top} f_1(W^1 x)) \prod_{i=1}^b h_i(w_{i}^{\top} x)\right] \\
        &\quad =\E h_0(u^{\top} f_1(W^1 x)) \prod_{i=1}^b \E h_i(w_{i}^{\top} x) + O(d^{-1/2})
        \end{aligned}
    \end{equation}
\end{proof}

\subsection{One layer linearization}
Consider a mean-zero Gaussian random vector $x \in \R^d$ with covariance $\E x x^\top =I$, two weight matrices $W\in \R^{k\times d}, V\in\R^{s\times d}$ and two smooth odd functions $f,g$ applied entrywise to $Wx,Vx$. 
We assume that rows of \(W\) and \(V\) are mean-zero i.i.d. samples \((w_i, v_i) \sim (w, v)\), such that \(C_w \coloneqq \E ww^{\top}\) and \(C_v \coloneqq \E vv^{\top}\). Let \(C_{wv} = \E w v^{\top}\) if \(s = k\) and \(C_{wv} = \mathbf{0}_{d \times d}\) (all-zero matrix) otherwise.

Let \(N_w, N_v\) be jointly Gaussian mean-zero random variables, such that 
\begin{equation}
    \E N_w^2 = \Tr C_w,\quad \E N_v^2 = \Tr C_v,\quad \E N_w N_v = \Tr C_{wv}.
\end{equation}

Define 
\begin{equation}
    \begin{aligned}
        &\Phi_1 = \E f(Wx) g(Vx)^{\top}, \\
        &\Phi_1^{\mathrm{lin}} = (\E f'(N_w))(\E g'(N_v)) W V^\top \\
        &\qquad \quad + [\E f(N_w)g(N_v)-(\E f'(N_w))(\E g'(N_v))(\E N_w N_v)] I.
    \end{aligned}
\end{equation}
\begin{proposition}
\label{prop:lin_1_layer}
    We have that, with high probability, \(
        \norm{\Phi_1 - \Phi_1^{\mathrm{lin}}}_F = O(1)\).
\end{proposition}

\begin{proof}

Using a Wiener chaos expansion (\cref{eq:wiener_chaos_expansion}), we can write
\begin{equation}
    \begin{aligned}
    f(W x)_i &= \sum_{p\ge 1} \frac{\E f^{(p)}((W W^\top)_{ii}^{1/2} N) }{p!} I_p(Wx)_i,\\
    g(V x)_j &= \sum_{p\ge 1} \frac{\E g^{(p)}((V V^\top)_{jj}^{1/2} N) }{p!} I_p(Vx)_j
    \end{aligned}
\end{equation}
where \(N \sim \cN(0, 1)\) and $I_p(Wx),I_q(Vx)$ are random vectors with covariance
\begin{equation}
    \E I_p(Wx) I_q(Vx)^\top = p! \delta_{pq}  (W V^\top)^{\odot p}
\end{equation}
with $A^{\odot p}$ denoting the $p$-th entrywise (Hadamard) power. Thus we have the identity
\begin{equation}
    \begin{aligned}
    &\E f(Wx)_i g(Vx)_j \\
    &\quad = \sum_{p\geq 1} \frac{1}{p!} (\E f^{(p)}((W W^\top)_{ii}^{1/2} N) ) (W  V^\top )_{ij}^{p} ( \E g^{(p)}((V V^\top)_{jj}^{1/2} N) ).
    \end{aligned}
\end{equation}
From~\cref{thm:hanson_wright} (note that \(\norm{w}_{\psi_2} \sim d^{-1/2}\) and same for \(v\)), and since \(\Tr C_w  \sim 1\), it follows that 
\begin{gather}
        (W W^\top)_{ii} = \Tr C_w  + O(d^{-1/2}), \quad (V V^\top)_{jj} = \Tr C_{v} + O(d^{-1/2}), \\
        (W V^\top)_{ij} = \delta_{ij}\Tr C_{wv} + O(d^{-1/2}).
\end{gather}
From perturbation analysis, we can write
\begin{equation}
    \begin{aligned}
    \E f^{(p)}((W  W^{\top})_{ii}^{1/2} N) &= \E f^{(p)}(\sqrt{\Tr C_w } N) + O(d^{-1/2}) \\
    &= \E f^{(p)}(N_w) + O(d^{-1/2}),
    \end{aligned}
\end{equation}
and similarly \(\E g^{(p)}((V  V^{\top})_{jj}^{1/2} N) = \E g^{(p)}(N_v) + O(d^{-1/2})\).
\paragraph{Off-diagonal entries}
Here, for \(p \geq 2\), we have that \((W V^{\top})^p_{ij} = O(d^{-p/2})\).
Therefore, 
\begin{equation}
    \E f(Wx)_i g(Vx)_j = \E f'(N_w) g'(N_v) (W V^{\top})_{ij} + O(d^{-1}) = (\Phi_1^{\mathrm{lin}})_{ij} + O(d^{-1}).
\end{equation}
\paragraph{Diagonal entries}
If \(s \neq k\), we have \((W  V^{\top})^p_{ii} = O(d^{-p/2})\) for \(p \geq 2\), and thus obtain the same expression as in previous case. When \(s = k\), we can rewrite the infinite sum as
\begin{equation}
\begin{aligned}
&\E f(Wx)_i g(Vx)_i \\
&\quad = \sum_{p\geq 1} \frac{1}{p!} (\E f^{(p)}((W W^\top)_{ii}^{1/2} N) ) (W  V^\top )_{ii}^{p} ( \E g^{(p)}((V V^\top)_{ii}^{1/2} N) ) \\
    &\quad = \sum_{p\geq 1}\frac{[\E f^{(p)}(\sqrt{\Tr C_w} N)][\E g^{(p)}(\sqrt{\Tr C_v} N)]}{p!} (\Tr C_{wv})^p + O(d^{-1/2}) \\
    &\quad = \E f(N_w)g(N_v) + O(d^{-1/2}) = (\Phi_1^{\mathrm{lin}})_{ii} + O(d^{-1/2}).
\end{aligned}
\end{equation}
Summing up over all entries, we conclude that \(\norm{\Phi_1 - \Phi_1^{\mathrm{lin}}}_F = O(1)\).
\end{proof}
Note that in case of independent \(N_v, N_w\) (i.e., independent \(v, w\)) the second term of \(\Phi_1^{\mathrm{lin}}\) vanishes and in case of $W = V$, \(f \equiv g\) this reduces to 
\begin{equation}
    \Phi_1^{\mathrm{lin}} =  (\E f'(N_w))^2 W W^\top + [\E f(N_w)^2-(\E f'(N_w))^2\Tr C_w] I.
\end{equation}
\subsection{Two layer case}
\label{app:second_layer}
We now consider the  $2$-layer example
\begin{equation}
    f_2(W^2 f_1(W^1 x)), \quad g_2(V^2 g_1(V^1 x)),
\end{equation}
with smooth odd\footnote{For brevity we present the full proof in the case of odd activation functions. The argument for the general case (i.e., when only assuming that activation functions are centered w.r.t. Gaussian distribution) is similar, but requires more tedious estimates.} functions $f_1,f_2,g_1, g_2$. We assume that the rows of \(W^1, W^2, V^1, V^2\) are mean-zero i.i.d. samples \((w^1_i, w^2_i, v^1_i, v^2_i) \sim (w^1, w^2, v^1, v^2)\), such that \(C_1 \coloneqq \E w^1(w^1)^{\top}, C_2 \coloneqq \E w^2(w^2)^{\top}, \tilde C_1 \coloneqq \E v^1(v^1)^{\top}\), and \(\tilde C_1 \coloneqq \E v^2(v^2)^{\top}\). Let \(\check C_1 = \E w^1 (v^1)^{\top}\) and \(\check C_2 =\E w^2 (v^2)^{\top}\). Let \((N_1, \wt N_1)\) be a zero-mean jointly Gaussian random variables:
\begin{equation}
    (N_1, \tilde N_1) \sim \cN \left(0,  \begin{pmatrix} \Tr(C_1) & \Tr(\check C_1)\\ \Tr(\check C_1) & \Tr(\tilde C_1) \end{pmatrix} \right),
\end{equation}
and define
\begin{equation}
\begin{aligned}
    &\Phi_1 \coloneqq \E f_1(W^1 x) g_1(V^1x)^\top, \\
    &\Phi_1^{\mathrm{lin}} = (\E f_1'(N_1))(\E g_1'(\wt N_1)) W^1 (V^1)^\top \\
    &\qquad \quad + [\E f_1(N_1)g_1(\wt N_1)-(\E f_1'(N_1))(\E g_1'(\wt N_1))(\E N_1 \wt N_1)] I. \\
\end{aligned}
\end{equation}
Similarly, we define \(\Omega_1, \Omega_1^{\mathrm{lin}}\) with \(V^1, g_1, \wt N_1\) replaced by \(W^1, f_1, N_1\) and we define \(\Psi_1, \Psi_1^{\mathrm{lin}}\) with \(W^1, f_1,  N_1\) replaced by \(V^1, g_1, \wt N_1\) (see~\Cref{def: linearized_covs}).
Next, let \((N_2, \wt N_2)\) be a zero-mean jointly Gaussian random variables:
\begin{equation}
    (N_2, \tilde N_2) \sim \cN \left(0,  \begin{pmatrix} \Tr(C_2 \Omega_1^{\mathrm{lin}}) & \Tr (\check C_2 \Phi_1^{\mathrm{lin}})\\ \Tr (\check C_2 \Phi_1^{\mathrm{lin}}) & \Tr(\tilde C_2 \Psi_1^{\mathrm{lin}}) \end{pmatrix} \right),
\end{equation}
and define
\begin{equation}
\begin{aligned}
    &\Phi_2 \coloneqq \E f_2(W^2f_1(W^1 x)) g_2(V^2g_1(V^1x))^\top, \\
    &\Phi_2^{\mathrm{lin}} = (\E f_2'(N_2))(\E g_2'(\wt N_2)) W^2 \Phi_1^{\mathrm{lin}}(V^2)^\top \\
    &\qquad \quad + [\E f_2(N_2)g_2(\wt N_2)-(\E f_2'(N_2))(\E g_2'(\wt N_2))(\E N_2 \wt N_2)] I, \\
\end{aligned}
\end{equation}
and, again, similarly \(\Omega_2, \Omega_2^{\mathrm{lin}}, \Psi_2,\) and  \(\Psi_2^{\mathrm{lin}}\).
% \Psi_i^{\mathrm{lin}}\) only with \(f_i\) and only with \(g_i\) respectively (see~\Cref{def: linearized_covs}). Scalar random variables \(N_1, \tilde N_1\) and \(N_2, \tilde N_2\) are zero-mean jointly Gaussian:
%     \begin{equation}
%         (N_1, \tilde N_1) \sim \cN \left(0,  \begin{pmatrix} \Tr(C_1) & \Tr(\check C_1)\\ \Tr(\check C_1) & \Tr(\tilde C_1) \end{pmatrix} \right) \quad \text{and} \quad 
%         (N_2, \tilde N_2) \sim \cN \left(0,  \begin{pmatrix} \Tr(C_2 \Omega_1^{\mathrm{lin}}) & \Tr (\check C_2 \Phi_1^{\mathrm{lin}})\\ \Tr (\check C_2 \Phi_1^{\mathrm{lin}}) & \Tr(\tilde C_2 \Psi_1^{\mathrm{lin}}) \end{pmatrix} \right).
% \end{equation}
% \begin{equation}
% \begin{aligned}
%     &\E N_1^2 = \Tr(C_1) \quad
%     &\E \wt N_1^2 = \Tr(\tilde C_1) \quad 
%     &\E N_1 \wt N_1 = \Tr(\check C_1), \\
%     &\E N_2^2 = \Tr(C_2 \Omega_1^{\mathrm{lin}}) \quad &\E \tilde N_2^2 = \Tr(\tilde C_2 \Psi_1^{\mathrm{lin}}) \quad &\E N_2 \tilde N_2 = \Tr (\check C_2 \Phi_1^{\mathrm{lin}}).
% \end{aligned}
% \end{equation}
\begin{theorem}
\label{thm:lin_app}
    We have that \(\norm{\Phi_2 - \Phi_2^{\mathrm{lin}}}_F \prec 1\).
\end{theorem}
We split the proof into the following lemmas:
\begin{lemma}[Diagonal entries of \(\Phi_2\)]
\label{lem:diag}
For possibly correlated vectors \(u, z\), we have the bound
    \begin{equation}
        \abs{\E f_2(u^{\top} f_1(W^1 x)) g_2(z^{\top} g_1(V^1 x)) - \E f_2(N_2)g_2(\tilde N_2)} \prec d^{-1/2}.
    \end{equation}
\end{lemma}
% \begin{lemma}[Diagonal entries of \(\Omega_2\)]
% \label{lem:diag}
%     \begin{equation}
%         \E f_2(u^{\top} f_1(W^1 x))^2 = \E f_2(N_2)^2 + O(d^{-1/2}).
%     \end{equation}
% \end{lemma}

\begin{lemma}[Off-diagonal entries of \(\Phi_2\)]
\label{lem:off_diag}
If \(u\) and \(z\) are independent, we have
    \begin{equation}
        \abs{\E f_2(u^{\top} f_1(W^1x)) g_2(z^{\top} g_1(V^1x)) - (\E f_2'(N_1)) (\E g_2'(N_2)) u^\top \Phi_1^{\mathrm{lin}} z} \prec d^{-1}.
    \end{equation}
\end{lemma}

% \begin{lemma}[Entries of \(\Phi_2\)]
% \label{lem:cross_terms}
%     \begin{equation}
%         \E f_2(u^\top f_1(W^1 x)) g(v^\top x) = \E \bigl[f_2'(u^\top f_1(Wx))] u^\top \E f_1(W x) g(v^\top x) + O(d^{-1}).
%     \end{equation}
% \end{lemma}

\begin{proof}[Proof of~\cref{thm:lin_app}]
    The proof follows from~\Cref{lem:diag,lem:off_diag} upon summation over all entries.
\end{proof}

\begin{proof}[Proof of~\Cref{theo lin}]
The proof follows from~\Cref{prop:lin_1_layer} and~\Cref{thm:lin_app}. Note that results for \(\Omega_i\) and \(\Psi_i\) can be obtained using aforementioned results only for \(f_i, W^i\) and only for \(g_i, V^i\) respectively.
\end{proof}

% \begin{proof}[Proof of~\cref{thm:lin_app} \dd{to remove}]
%     \cref{lem:cross_terms} implies that
%     \begin{equation}
%         \norm{\Phi_2 - \E \bigl[f_2'(u^\top f_1(Wx))] W^2 \Phi_1}_F = O(1).
%     \end{equation}
%     Note that, since \(\norm{\Phi_1 - \Phi_1^{\mathrm{lin}}}_F = O(1)\) and since \(\norm{W^2} = O(1)\), we have that 
%     \begin{equation}
%         \norm{\E \bigl[f_2'(u^\top f_1(Wx))\bigr] W^2 \Phi_1 - \E \bigl[f_2'(u^\top f_1(Wx))\bigr] W^2 \Phi_1^{\mathrm{lin}}}_F = O(1),
%     \end{equation}
%     therefore, by triangle inequality,
%     \begin{equation}
%         \norm{\Phi_2 - \E \bigl[f_2'(u^\top f_1(Wx))\bigr] W^2 \Phi_1^{\mathrm{lin}}}_F = O(1).
%     \end{equation}
%     Finally, note that by simple chaos expansion, \(\E \bigl[f_2'(u^\top f_1(Wx))\bigr] = \E f_2'(N_2)\), therefore \(\norm{\Phi_2 - \Phi_2^{\mathrm{lin}}}_F = O(1)\).

%     Furthermore, \cref{lem:diag} together with \cref{lem:off_diag} imply that
%     \begin{equation}
%         \norm{\Omega_2 - (\E\bigl[f_2'(u^\top f_1(Wx))\bigr])^2 W^2 \Omega_1 (W^2)^{\top} - (\E f_2(N_2)^2 - (\E\bigl[f_2'(u^\top f_1(Wx))\bigr])^2)I}_F = O(1).
%     \end{equation}
%     Again, using the fact that \(\norm{\Omega_1 - \Omega_1^{\mathrm{lin}}}_F = O(1)\), \(\norm{W^2} = O(1)\) and approximating \(\E f_2'(u^\top f_1(Wx))\) with \(\E f_2'(N_2)\), we obtain that \(\norm{\Omega_2 - \Omega_2^{\mathrm{lin}}}_F = O(1)\).
% \end{proof}

% \subsection{Proof of~\cref{lem:diag}}
% % \begin{lemma}
% %     \begin{equation}
% %         \E f_2(u^{\top} f_1(W^1 x))^2 = \E N_{2, w}^2 + O(d^{-1/2}).
% %     \end{equation}
% % \end{lemma}
% Let \(F = u^{\top} f_1(W^1 x)\). Our goal is to compute \(\E f_2(u^{\top} f_1(W^1 x))^2 = \E f_2(F)^2\).
% For simplicity we omit indices in \(f_1\) and \(W^1\) and write just \(f\) and \(W\).
% We can decompose
% \begin{equation}
%     F = u^{\top} f(Wx) = \sum_p I_p\left(\frac{\E D^p F}{p!}\right) = \sum_p I_p \left(\sum_i \frac{v_i \E D^p f(w_i^\top x)}{p!}\right) = \sum_p I_p \left(\sum_i \frac{v_i w_i^{\otimes p} \E f^{(p)}(w_i^\top x)}{p!}\right).
% \end{equation}
% Let \(f^p_i \coloneqq \E f^{(p)} (w_i^\top x)\). We obtain that
% \begin{equation}
%     DF = \sum_{p \geq 1} p I_{p - 1}\left(\sum_i \frac{v_i w_i^{\otimes p} f^p_i}{p!}\right)
%     \quad \text{and} \quad -D L^{-1} F = \sum_{q \geq 1} I_{q - 1}\left(\sum_i \frac{v_i w_i^{\otimes q} f^q_i}{q!}\right).
% \end{equation}
% \begin{lemma}
%     \begin{equation}
%         \E \abs{\braket{DF, -DL^{-1}F} - \E F^2} = O(d^{-1/2}).
%     \end{equation}
% \end{lemma}
% \begin{proof}
% Note that 
% \begin{equation}
%     I_{p-1}\left(\sum_i \frac{v_i w_i^{\otimes p} f^p_i}{p!}\right)
%     = \sum_i \frac{v_i f^p_i I_{p-1}(w_i^{\otimes p - 1})w_i}{p!},
% \end{equation}
% which implies that, for some coefficients \(c_{p, q}\), 
% \begin{equation}
% \label{eq:dl_dlinv_iprod1}
% \braket{DF, -DL^{-1} F} = \sum_{p,q\geq 1} c_{p,q} \sum_{i,j} \braket{w_i, w_j} v_i v_j f^p_i f^q_j I_{p-1}(w_i^{\otimes p - 1}) I_{q - 1}(w_j^{\otimes q - 1}).
% \end{equation}
% Now, using (\cite{nourdin2012normal}, Theorem 2.7.10), we can rewrite
% \begin{equation}
% \begin{aligned}
%     &I_{p-1}(w_i^{\otimes p - 1}) I_{q - 1}(w_j^{\otimes q - 1}) \\
%     &\quad =
%     \sum_{r=0}^{p \land q - 1} \braket{w_i, w_j}^r c_{r,p,q} I_{p + q - 2(r+1)}(w_i^{\otimes p - 1 - r} \wt \otimes w_j^{\otimes q - 1 - r}) \\
%     &\quad = \sum_{\substack{s = \abs{p-q} \\ 2 \text{ divides } (s - \abs{p - q})}}^{p+q - 2} c_{r,p,q}\braket{w_i, w_j}^{(p+q-2-s)/2} I_s \left(w_i^{\otimes (p-q+s)} \wt \otimes w_j^{\otimes (q -p+s)}\right),
% \end{aligned}
% \end{equation}
% and plugging this expression back into~\cref{eq:dl_dlinv_iprod1}, we get
% \begin{equation}
%     \braket{DF, -DL^{-1}F} = \sum_{s \geq 0} \sum_{\substack{\abs{p -q} \leq s \\ 2 \text{ divides } (s - \abs{p - q}) \\ p \land q \geq 1 + (s - \abs{p - q}) / 2}} \tilde c_{r,p,q}\sum_{i, j}\braket{w_i, w_j}^{(p+q - s)/2} v_i v_j f^p_i f^q_j 
%     I_s(w_i^{\otimes (s + p - q) / 2} \wt \otimes w_j^{\otimes (s + q - p) / 2}).
% \end{equation}
% In the sum above, term \(s = 0\) corresponds to \(\E F^2\).
% Let us collect all the terms corresponding to \(s\)th multiple integral.
% Note that given conditions on \(s, p, q\), we always have that \((p+q - s)/2 \geq 1\), which is the power of the inner product \(\braket{w_i, w_j}\) in the expression above. If we introduce \(a \coloneqq (p+q - s) / 2\), then for fixed \(s\), \(s\)th multiple integral \(I_s\) can be rewritten as follows:
% \begin{equation}
%     I_s\left(\sum_{a \geq 1} \sum_{i, j} \braket{w_i, w_j}^a v_i v_j T^s_{ij}\right),
% \end{equation}
% where \(T^s_{ij}\) is some \(s\)-dimensional tensor, which is a sum of tensor products of \(w_i\) and \(w_j\), also containing combinatorial terms, and products of expectations of derivatives of \(f\). 
% Then, we can write
% \begin{equation}
%     \E (\braket{DF, -DL^{-1}F} - \E F^2)^2 = \sum_{s \geq 1} \E I_s\left(\sum_{a \geq 1} \sum_{i,j} \braket{w_i, w_j}^a v_i v_j T^s_{ij}\right)^2.
% \end{equation}
% Observe that
% \begin{equation}
% \label{eq:I_s_square}
%     \E I_s\left(\sum_{a \geq 1} \sum_{i,j} \braket{w_i, w_j}^a v_i v_j T^s_{ij}\right)^2 = 
%     \sum_{a, a' \geq 1} \sum_{\substack{i, j \\i', j'}} \braket{w_i, w_j}^a \braket{w_{i'}, w_{j'}}^{a'} v_i v_j v_{i'} v_{j'} \braket{T^s_{ij}, T^s_{i'j'}},
% \end{equation}
% and note that for some constant \(C\) (depending on combinatorial terms, and products of expectations of derivatives of \(f\)) \(\braket{T^s_{ij}, T^s_{i'j'}}\) can be upper bounded by
% \begin{equation}
%     \braket{T^s_{ij}, T^s_{i'j'}} \leq C (\braket{w_i, w_{i'}} + \braket{w_i, w_{j'}} + \braket{w_j, w_{i'}} + \braket{w_j, w_{i'}})^s.
% \end{equation}
% Now, we analyze each term of the summand in~\cref{eq:I_s_square} depending on \(a, a', i, i', j, j'\). 
% Define \(N \coloneqq \abs{\{i, i', j, j'\}}\), the number of distinct indices among \(i, i', j\) and \(j'\).
% Note that since entries of \(v\) are of order \(v_i \lesssim d^{-1/2}\), we get that in total \(v_iv_jv_{i'}v_{j'}\) contribute \(O(d^{-2})\).
% \paragraph{Case \(N = 1\)}
% Here, since there are in total \(d\) such terms, which immediately obtain \(O(d^{-1})\) upper bound.
% \paragraph{Case \(N = 2\)}
% There are in total \(O(d^2)\) such terms.
% In this case, it must be that either (1) \(i \neq j\) or (2) \(i' \neq j'\) or (3) both \(i = j\), \(i' = j'\).
% Note that in the first two cases, we get that \(\braket{w_i, w_j}^a \braket{w_{i'}, w_{j'}}^{a'} \lesssim d^{-1/2}\),
% which together with bound on \(v_i\)'s gives \(O(d^{2 - 2 - 1/2})\) contribution.
% If both \(i = j\) and \(i' = j'\), then necessarily \(\braket{T_{ij}^s, T_{i'j'}^s} = O(d^{-1/2})\) and we arrive at the same conclusion.
% \paragraph{Case \(N = 3\), \(\min(a, a', s) \geq 2\)}
% There are in total \(O(d^3)\) such terms. WLOG assume that \(i = j\). Since \(\min(q, q', s) \geq 2\),
% we get that \(\braket{w_{i'}, w_{j'}}^{q'} = O(d^{-1})\) and \(\braket{T_{ij}^s, T_{i'j'}^s}^{s'} = O(d^{-1})\), which in total gives \(O(d^{-1})\) contribution.
% \paragraph{Case \(N = 4\)}
% Here, there are in total \(O(d^4)\) such terms.
% If \(\min(a, a', s) \geq 2\), then it total we obtain \(O(d^{4-2-3}) = O(d^{-1})\) contribution.
% We analyze case \(a = a' = s = 1\), and others follow by similar arguments.
% Let \(X = \sum_{\substack{i, j, i', j' \\ \text{all distinct}}} \braket{w_i, w_j} \braket{w_{i'}, w_{j'}} v_i v_j v_{i'} v_{j'} \braket{T^1_{ij}, T^1_{i'j'}}\). Note that \(\E_W X = 0\) and 
% \begin{equation}
%     X^2 = \sum_{\substack{i_1, j_1, i'_1, j'_1 \\ \text{all distinct}}} \sum_{\substack{i_2, j_2, i'_2, j'_2 \\ \text{all distinct} }}\braket{w_{i_1}, w_{j_1}} \braket{w_{i_1'}, w_{j_1'}} \braket{w_{i_2}, w_{j_2}} \braket{w_{i_2'}, w_{j_2'}} v_{i_1} v_{j_1} v_{i'_1} v_{j'_1} v_{i_2} v_{j_2} v_{i'_2} v_{j'_2} \braket{T^1_{i_1j_1}, T^1_{i_1'j_1'}} \braket{T^1_{i_2j_2}, T^1_{i_2'j_2'}}.
% \end{equation}
% The only non-zero contribution comes from terms with pairings between indices, therefore, we get that \(\E_W X^2 = O(d^{-1})\), which implies that \(X = O(d^{-1/2})\).
% \paragraph{Case \(N = 3\), \(\min(a, a', s) = 1\)}
% This case can be done similarly to the previous ones.
% % Again, there are in total \(O(d^3)\) such term. 
% % WLOG assume that \(a = 1\). 
% % If \(i = j\), then either \(\min(a', s) = 1\), and we can by changing indices assume that \(i \neq j\), 
% % or \(\min(a', s) \geq 2\), and then by the previous case we get that total contribution is \(O(d^{-1})\).
% % Now, we can assume that \(a = 1\) and \(i \neq j\).
% % Let \(X = \braket{w_i, w_j} \braket{w_{i'}, w_{j'}}^{a'} v_i v_j v_{i'} v_{j'} \braket{T^s_{ij}, T^s_{i'j'}}\) be a term under consideration. \dd{todo}
% \end{proof}
% Overall, we obtain that 
% \begin{equation}
%     \E (\braket{DF, -DL^{-1} F} - \E F^2)^2 = O(d^{-1/2}),
% \end{equation}
% which, by~\cref{thm:clt}, using \(h(x) = f_2(x)^2\) that 
% \begin{equation}
%     \E f_2(u^\top f_1(W^1 x))^2 = \E f_2(Z)^2 + O(d^{-1/2}),
% \end{equation}
% where \(\E Z^2 = u^\top \E f_1(W^1 x) u\). By perturbation analysis, we obtain that 
% \begin{equation}
%     \E f_2(u^\top f_1(W^1 x))^2 = \E f_2(N_2)^2 + O(d^{-1/2})
% \end{equation}
% \subsection{Proof of~\cref{lem:cross_terms}}
% We restate the lemma:
% \begin{lemma}
%     \begin{equation}
%         \E f_2(u^\top f_1(Wx)) g(v^\top x) = \E \bigl[f_2'(u^\top f_1(Wx))] u^\top \E f_1(W x) g(v^\top x) + O(d^{-1})
%     \end{equation}
% \end{lemma}
% Our goal is to compute
% \begin{equation}
%     \E f_2(u^\top f_1(Wx)) g(v^\top x) .
% \end{equation}
% Let \(F_i = f_1(w_i^\top x)\) and \(F = (F_1, \ldots, F_n)\), where \(w_i\)'s are the rows of \(W\).
% Note that we can view \(f_2(u^\top f_1(Wx)) = \phi(F)\), for \(\phi(F) = f_2(\sum u_k F_k)\).
% Recall that \(D^p F_k = f^{(p)}(w_k^\top x) w_k^{\otimes p}\).
% Then, we have that 
% \begin{equation}
%     D \phi(F) = \sum_k f_2'(u^\top f_1(Wx)) u_k D F_k.
% \end{equation}
% From~\cref{lemma:weak_corr}, it follows that
% \begin{equation}
%     \E D \phi(F) = \sum_k \E \bigl[f_2'(u^\top f_1(Wx))\bigr] u_k \E D F_k + O(d^{-1/2}),
% \end{equation}
% and we obtain that 
% \begin{equation}
%     \braket{\E D g(v^{\top} x), \E D f_2(u^\top f_1(Wx))} = \E \bigl[f_2'(u^\top f_1(Wx))\bigr] \sum_k u_k \braket{\E D g(v^{\top} x), \E D f_1(w_i^\top x)}.
% \end{equation}
% For the second derivative, we obtain that 
% \begin{equation}
%     D^2 \phi(F) = \sum_{k, k'} f_2''(u^\top f_1(Wx)) u_k u_k' (D F_k) \otimes (D F_{k'}) 
%     + \sum_k f_2'(u^\top f_1(Wx)) u_k D^2 F_k.
% \end{equation}
% Overall, for \(\E D^p \phi(F)\), we obtain
% \begin{equation}
% \begin{aligned}
%     \E D^p \phi(F) = \sum_k \E \left[f_2'(u^\top f_1(Wx)) u_k D^p F_k\right] + \E R
% \end{aligned}
% \end{equation}
% where \(R\) is the term containing higher derivatives of \(f_2\).
% This implies that 
% \begin{equation}
% \braket{\E D^p \phi(F), \E D^p g(v^\top x)} = \sum_{k} u_k \E\left[f'_2(u^\top f_1(Wx)) f^{(p)}(w_k^\top x)\right]\E \left[g^{(p)}(v^\top x)\right] \braket{w_k, v}^p + \braket{\E R, \E D^p g(v^\top x)}.
% \end{equation}

% From~\cref{lemma:weak_corr}, we have that
% \begin{equation}
%      \E\left[f'_2(u^\top f_1(Wx)) f^{(p)}(w_k^\top x)\right] =  \E\left[f'_2(u^\top f_1(Wx))\right] \E\left[f^{(p)}(w_k^\top x)\right] + Q,
% \end{equation}
% where \(Q = O(d^{-1/2})\). We now show that terms involving \(Q\) have \(O(d^{-1})\) contribution to the final expression:
% \begin{lemma}
% \label{lem:error_from_weak_corr}
%     \begin{equation}
%         \sum_{k} u_k \braket{w_k, v}^p = O(d^{-{1/2}})
%     \end{equation}
% \end{lemma}
% \begin{proof}
%     Since \(v\) can be correlated with at most one row of \(W\) by our assumptions, WLOG we assume that \(v\) is uncorrelated with all \(w_k\) for \(k \geq 2\).
%     Then we can write 
%     \begin{equation}
%         \sum_{k} u_k \braket{w_k, v}^p = u_1 \braket{w_1, v}^p + \sum_{k \geq 2} u_k \braket{w_k, v}^p,
%     \end{equation}
%     where the first term is \(O(d^{-1/2})\) since \(u_i = O(d^{-1/2})\). Next, if \(p \geq 2\), we directly obtain that the second term is \(O(d^{-1/2})\).
%     For case \(p = 1\), note that since \(\E_u \left[\sum_{k \geq 2} u_k \braket{w_k, v}^p\right] = 0\) and 
%     \begin{equation}
%         \E_u \left[\left(\sum_{k \geq 2} u_k \braket{w_k, v}\right)^2\right] = \E_u \left[\sum_{k \geq 2} u_k^2 \braket{w_k, v}^2\right] = O(d^{-1}),
%     \end{equation}
%     we obtain that \(\sum_{k} u_k \braket{w_k, v}^p = O(d^{-{1/2}})\) with high probability.
% \end{proof}
% \cref{lem:error_from_weak_corr} implies that 
% \begin{equation}
% \begin{aligned}
%     &\braket{\E D^p \phi(F), \E D^p g(v^\top x)} \\
%     &\quad = \sum_{k} u_k \E\left[f'_2(u^\top f_1(Wx))\right]
%     \E \left[f^{(p)}(w_k^\top x)\right]\E \left[g^{(p)}(v^\top x)\right] \braket{w_k, v}^p + \braket{\E R, \E D^p g(v^\top x)} + O(d^{-1}) \\
%     &\quad = \E\left[f'_2(u^\top f_1(Wx))\right] \sum_{k} u_k \braket{\E D^p F_k, \E D^p g(v^\top x)} + \braket{\E R, \E D^p g(v^\top x)} + O(d^{-1}).
% \end{aligned}
% \end{equation}
% \begin{lemma}
% \label{lem:remainder_term}
%     \begin{equation}
%         \braket{\E R, \E D^p g(v^\top x)} = O(d^{-1}).
%     \end{equation}
% \end{lemma}
% \begin{proof}
% Note that we can rewrite 
% \begin{equation}
%     \braket{\E R, \E D^p g(v^\top x)} = \sum_{\substack{\pi \vdash[p] \\ \pi \neq \{[p]\}}} \sum_{i_1, \ldots, i_{\abs{\pi}}} \prod_{q=1}^{\abs{\pi}} \left(u_{i_q} \braket{v, w_{i_q}}^{b(q)}\right), 
% \end{equation}
% where for \(\pi\) (partition of \([p]\)) we denote \(b(q)\) as the size of \(q\)th block.
% Let \(\pi \neq \{[p]\}\) be some partition, \(s \coloneqq \abs{\pi}\), and let 
% \(A_{\pi} \coloneqq \sum_{i_1, \ldots, i_{s}} \prod_{q=1}^{s} \left(u_{i_q} \braket{v, w_{i_q}}^{b(q)}\right)\).
% If all blocks of \(\pi\) are of size at least 2, then naive estimate gives in total the contribution is \(O(d^{-1})\).
% % Furthermore, if two indices of the summation coincide, we again arrive at total contribution \(O(d^{-1})\).
% Now assume that there is a block of size 1.
% WLOG we assume that \(v\) is correlated with \(w_1\) and \(b(1) = 1\).
% If \(i_1 = 1\), then let \(r\) be the number of indices among \(s - 1\) remaining indices, such that corresponding index is summed over \(i_k \geq 2\).
% If \(r \leq s - 2\), then we arrive at the estimate \(O(d^{r - s/2 - r/2}) = O(d^{-1})\).
% Now, assume that \(r = s - 1\). If two indices coincide, then we arrive at the estimate \(O(d^{(s - 2) - s/2 - (s - 1)/2}) = O(d^{-1})\).
% Therefore, the remaining case is
% \begin{equation}
%     B_{\pi} \coloneqq \sum_{i_2 \neq i_3 \neq \ldots \neq i_{s} \neq 1} u_{1} \braket{v, w_1} \prod_{q=2}^{s} u_{i_q} \braket{v, w_{i_q}}^{b(q)}. 
% \end{equation}
% If exists \(q > 1\), such that \(b(q) \geq 1\), then from naive estimate we obtain bound \(O(d^{s - 1 - s/2 - (s - 2) / 2 - 1}) = O(d^{-1})\).
% Therefore, we can assume that all blocks of \(\pi\) are of size 1.
% By independence of rows, the $W$-expectation of \(B_{\pi}\) is zero and
% \begin{equation}
%     \E_W B_{\pi}^2 = \sum_{i_2 \neq i_3 \neq \ldots \neq i_{\abs{\pi}} \neq 1} 
%     \sum_{i'_2 \neq i'_3 \neq \ldots \neq i'_{\abs{\pi}} \neq 1} u_{1}^2  \braket{v, w_1}^2 \prod_{q=2}^{p} u_{i_q} u_{i'_q} \braket{v, w_{i_q}} \braket{v, w_{i'_q}}.
% \end{equation}
% The only non-zero contributions come from pairings between \(i, i'\), which contribute \(O(d^{(p - 1) - p - (p - 1)}) = O(d^{-1})\).
% The case \(i_1 \neq 1\) follows by similar calculations. Overall, we proved that \(\braket{\E R, \E D^p g(v^\top x)} = \sum_{\pi \neq \{[p]\}} O(d^{-1}) = O(d^{-1})\).
% \end{proof}

% %     We now estimate the error term which consists of terms of the type 
% % \begin{equation}\label{error term 21}
% %     \sum_{k_1,\ldots,k_s} E_k u_{k_1}\cdots u_{k_s} \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}},
% % \end{equation}
% % where $s\ge 2$ and $E_k$ is some term consisting of combinatorial factors and the expectations of $f_2^{(s)}$ and derivatives of $g,f_1$. In case that $v$ is not independent of some (at most $1$) $w_k$, which without loss of generality we assume to be $w_1$, we split~\Cref{error term 21} according to how many indices $k_i$ are summed over $k_i\ge 2$, and how many are fixed to $1$. If $r<s$ indices are summed, then we have the naive estimate $d^{r-s/2-r/2}=d^{(r-s)/2}\le d^{-1/2}$, hence we can focus on the part of the summation where all indices are summed over $k_i\ge 2$. If two indices coincide, then we have the naive estimate $d^{-1}$, hence it is sufficient to estimate the part of the sum where all $k_1,\ldots,k_s$ are pairwise distinct, and distinct from $1$. By independence of rows, the $W$-expectation of this sum is identically zero and we continue by estimating its variance 
% % \begin{equation}
% %     \begin{split}
% %           &\E_W \biggl(\sum_{k_1\ne\cdots\ne k_s\ne 1}  E_k u_{k_1}\cdots u_{k_s} \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}}\biggr)^2 \\
% % &\quad = \sum_{k_1\ne\cdots\ne k_s\ne 1}\sum_{k'_1\ne\cdots\ne k'_s\ne 1}  E_k E_{k'} u_{k_1}\cdots u_{k_s} u_{k'_1}\cdots u_{k'_s} \E_W \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}} \braket{v,w_{k'_1}}\cdots \braket{v,w_{k'_s}}.
% %     \end{split}
% % \end{equation}
% % Note that the only non-zero contribution comes from the pairings between $k,k'$ which contribute $d^{s-2s}=d^{-s}$.
% % Since \(s \geq 2\), we get that the total expression can be bounded by \(O(d^{-2})\), and therefore, \(X = O(d^{-1})\).
% % \end{proof}
% Using~\cref{lem:remainder_term}, we have that \(\braket{\E D^p \phi(F), \E D^p g(v^\top x)} = \E\left[f'_2(u^\top f_1(Wx))\right] \sum_{k} u_k \braket{\E D^p F_k, \E D^p g(v^\top x)} + O(d^{-1})\), and this implies that
% \begin{equation}
% \begin{aligned}
%     &\E f_2(u^\top f_1(Wx)) g(v^\top x)  \\
%     &\quad = \sum_{p \geq 1} \frac{1}{p!} \braket{\E D^p f_2(u^{\top} f_1(Wx)), \E D^p g(v^{\top} x)} \\
%     &\quad = \sum_{p \geq 1} \frac{1}{p!} \sum_k u_k \E \bigl[f_2'(u^\top f_1(Wx))] \braket {\E D^p f_1(w_k^{\top} x), \E D^p g(v^\top x)} + O(d^{-1}) \\
%     &\quad = \E \bigl[f_2'(u^\top f_1(Wx))] \sum_k u_k \E f_1(w_k^{\top} x) g(v^\top x)  +  O(d^{-1}) \\
%     &\quad = \E \bigl[f_2'(u^\top f_1(Wx))] u^\top \E f_1(W x) g(v^\top x) + O(d^{-1}).
% \end{aligned}
% \end{equation}
% % We can write term \(R\) as 
% % \begin{equation}
% %     \sum_{\substack{\pi \vdash[p] \\ \pi \neq \{[p]\}}} \sum_{i_1, \ldots, i_{\abs{\pi}}} \prod_{q=1}^{\abs{\pi}} \left(u_{i_q} \braket{v, w_{i_q}}^{b(q)}\right), 
% % \end{equation}
% % where for \(\pi\) (partition of \([p]\)) we denote \(b(q)\) as the size of \(q\)th block.

% % We now estimate the error term which consists of terms of the type 
% % \begin{equation}\label{error term 21}
% %     \sum_{k_1,\ldots,k_s} E_k u_{k_1}\cdots u_{k_s} \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}},
% % \end{equation}
% % where $s\ge 2$ and $E_k$ is some term consisting of combinatorial factors and the expectations of $f_2^{(s)}$ and derivatives of $g,f_1$. In case that $v$ is not independent of some (at most $1$) $w_k$, which without loss of generality we assume to be $w_1$, we split~\Cref{error term 21} according to how many indices $k_i$ are summed over $k_i\ge 2$, and how many are fixed to $1$. If $r<s$ indices are summed, then we have the naive estimate $d^{r-s/2-r/2}=d^{(r-s)/2}\le d^{-1/2}$, hence we can focus on the part of the summation where all indices are summed over $k_i\ge 2$. If two indices coincide, then we have the naive estimate $d^{-1}$, hence it is sufficient to estimate the part of the sum where all $k_1,\ldots,k_s$ are pairwise distinct, and distinct from $1$. By independence of rows, the $W$-expectation of this sum is identically zero and we continue by estimating its variance 
% % \begin{equation}
% %     \begin{split}
% %           &\E_W \biggl(\sum_{k_1\ne\cdots\ne k_s\ne 1}  E_k u_{k_1}\cdots u_{k_s} \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}}\biggr)^2 \\
% % &\quad = \sum_{k_1\ne\cdots\ne k_s\ne 1}\sum_{k'_1\ne\cdots\ne k'_s\ne 1}  E_k E_{k'} u_{k_1}\cdots u_{k_s} u_{k'_1}\cdots u_{k'_s} \E_W \braket{v,w_{k_1}}\cdots \braket{v,w_{k_s}} \braket{v,w_{k'_1}}\cdots \braket{v,w_{k'_s}}.
% %     \end{split}
% % \end{equation}
% % Note that the only non-zero contribution comes from the pairings between $k,k'$ which contribute $d^{s-2s}=d^{-s}$.
% % Since \(s \geq 2\), we get that the total expression can be bounded by \(O(d^{-2})\), and therefore, \(X = O(d^{-1})\).

% Therefore, we obtain that \(\E f_2(u^\top f_1(Wx)) g(v^\top x) = \E \bigl[f_2'(u^\top f_1(Wx))] u^\top \E f_1(W x) g(v^\top x) + O(d^{-1})\)
% and, thus
% \begin{equation}
% \begin{aligned}
%     (\Phi_2)_{ij} = \E f_2((W^2_i)^\top f_1(W^1 x)) g(v_j^\top x)^{\top} = \mathrm{diag}\{\E f_2'(W^2 f_1(W^1 x))\} (W^2_i)^\top (\Phi_1^{\mathrm{lin}})_j + O(d^{-1}),
% \end{aligned}
% \end{equation}
% implying that \(\norm{\Phi_2 - \Phi_2^{\mathrm{lin}}} = O(1)\).

\subsection{Proof of~\cref{lem:off_diag}}
For simplicity of notation, we omit indices in \(W^1, V^1\) and write \(W, V\) instead. We begin with showing that
    \begin{equation}
        \begin{aligned}
        &\E f_2(u^{\top} f_1(Wx)) g_2(z^{\top} g_1(Vx)) \\
        &\quad = \E \left[f_2' (u^\top f_1(Wx))\right] \E \left[g_2' (z^\top g_1(Vx))\right]  u^\top \Phi_1 z + O(d^{-1}),
        \end{aligned}
    \end{equation}
for \emph{independent} random vectors \(u\) and \(z\).
Recall that 
\begin{equation}
    \begin{aligned}
    &f_2(u^\top f_1(Wx)) \\
    &\, = \sum_{p \geq 1} \frac{1}{p!} I_p \left(\underbrace{\E \sum_{\pi \vdash[p]} f_2^{(\Card{\pi})} (u^\top f_1(Wx)) \wt \bigotimes_{B \in \pi} \left(\sum u_k f_1^{(\Card{B})}(w_i^\top x) w_i^{\otimes \Card{B}}\right)}_{\E D^p f_2(u^\top f_1(Wx))}\right),
    \end{aligned}
\end{equation}
and that 
\begin{align*}
&\E f_2(u^{\top} f_1(Wx))g_2(z^{\top} g_1(Vx)) \\
&\quad = \sum_{p \geq 1} \frac{1}{p!} \braket{\E D^p f_2(u^{\top} f_1(Wx)), \E D^p g_2(z^{\top} g_1(Vx))}.
\end{align*}

Let \(f_2^{p} \coloneqq \E f_2^{(p)}(u^\top f_1(Wx))\) and \(g_2^{p} \coloneqq \E g_2^{(p)}(z^\top g_1(Vx))\). 
\Cref{lemma:weak_corr} implies that
    \begin{equation}
    \begin{aligned}
        &\Bigl\langle\E \left[f_2^{(\Card{\pi})}(u^\top f_1(W^1 x)) \wt \bigotimes_{B \in \pi} \left(
        \sum_k u_k f_1^{(\Card{B})} (w_i^\top x) w_i^{\otimes \Card{B}}\right)\right], \\
        &\quad \E \left[g_2^{(\Card{\pi'})}(z^\top g_1(V^1 x)) \wt \bigotimes_{B \in \pi'} \left(
        \sum_k u_k f_1^{(\Card{B})} (w_i^\top x) w_i^{\otimes \Card{B}}\right)\right]\Bigr\rangle \\
        & \quad = f_2^{\Card{\pi}} g_2^{\Card{\pi'}} 
        \Bigg\langle\wt\bigotimes_{B \in \pi} \left(
        \sum_k u_k f_1^{(\Card{B})} (w_i^\top x) w_i^{\otimes \Card{B}}\right),\\ 
        &\qquad \quad \wt\bigotimes_{B' \in \pi'} \left(
        \sum_k z_k g_1^{(\Card{B'})} (v_i^\top x) v_i^{\otimes \Card{B'}}\right)\Bigg\rangle + O(d^{-1}).
    \end{aligned}
    \end{equation}

Using it, we can write (denoting \(f_{1i}^p \coloneqq \E f_1^{(p)}(w_i^\top x)\) and \(g_{1i}^p \coloneqq \E g_1^{(p)}(v_i^\top x)\))
\begin{equation}
\begin{aligned}
    & \braket{\E D^p f_2(u^{\top} f_1(Wx)), \E D^p g_2(z^{\top} g_1(Vx))} \\
    & \quad = \sum_{\substack{\pi, \pi' \vdash [p]}} \sum_{\substack{i_1, \ldots, i_{\lvert \pi \rvert} \\
    j_1, \ldots, j_{\rvert \pi' \rvert}}} f_2^{\Card \pi} g_2^{\Card {\pi'}} \prod_{k = 1}^{\Card{\pi}} \left( f_{1i}^{b(k)} u_{i_k} \right) \prod_{k = 1}^{\Card{\pi'}} \left( g_{1i}^{b'(k)} z_{j_k} \right) \\
    &\qquad \qquad \qquad \qquad \cdot \frac{1}{p!} \sum_{\sigma \in S_p} \prod_{q=1}^p \braket{w_{i_{\pi(q)}}, v_{j_{\pi'(\sigma(q))}}} + O(d^{-1}),
\end{aligned}
\end{equation}
where \(b(k)\) and \(b'(k)\) denote the size of \(k\)th block in \(\pi\) and \(\pi'\) respectively.
The term \(\pi = \pi' = \{[p]\}\) corresponds to 
\begin{equation}
    \sum_{i, j} \E f_2' (u^\top f_1(Wx)) \E g_2'(z^\top g_1(Vx)) f_1^{(p)}(w_i^\top x)g_1^{(p)}(v_j^\top x)u_i z_j \braket{w_{i}, v_{j}}^p,
\end{equation}
which, after summing over \(p \geq 1\) is equal to
\begin{equation}
    \E \left[f_2' (u^\top f_1(Wx))\right] \E \left[g_2' (z^\top g_1(Vx))\right] u^\top \E \left[f_1(W^\top x)g_1(V^\top x)\right]z.
\end{equation}
Therefore, it remains to show that all the other terms contribute in total \(O(d^{-1})\). Note that \(f_2^{\Card{\pi}} = O(1)\) and same for other derivatives.
\begin{lemma}
    Fix \(\pi, \pi' \vdash [p]\). Then,
    \begin{equation}
       \sum_{\substack{i_1, \ldots, i_{\lvert \pi \rvert} \\
    j_1, \ldots, j_{\rvert \pi' \rvert}}}  \prod_{k = 1}^{\Card{\pi}}  u_{i_k} \prod_{k = 1}^{\Card{\pi'}}  z_{j_k} \prod_{q=1}^p \braket{w_{i_{\pi(q)}}, v_{j_{\pi'(q)}}} \prec d^{\frac{1}{2}(\min(\Card{\pi}, \Card{\pi'}) - p)}.
    \end{equation}
    
\end{lemma}
\begin{proof}
    Without loss of generality, it is enough to show upper bound \(\prec d^{\frac{1}{2}(\Card{\pi'} - p)}\). 
    By separating terms that depend on \(i_k\) for \(k \in [\Card{\pi}]\), we can rewrite
    \begin{equation}
        \begin{aligned}
        &\sum_{\substack{i_1, \ldots, i_{\lvert \pi \rvert} \\
    j_1, \ldots, j_{\rvert \pi' \rvert}}}  \prod_{k = 1}^{\Card{\pi}}  u_{i_k} \prod_{k = 1}^{\Card{\pi'}}  z_{j_k} \prod_{q=1}^p \braket{w_{i_{\pi(q)}}, v_{j_{\pi'(q)}}} \\
    &\quad = \sum_{j_1, \ldots, j_{\Card{\pi'}}} \prod_{k=1}^{\Card{\pi'}} z_{j_k} \prod_{k=1}^{\Card{\pi}} \left(\sum_{i_k} u_{i_k} \prod_{q: \pi(q) = k} \braket{w_{i_k}, v_{j_{\pi'(q)}}}\right).
        \end{aligned}
\end{equation}
    Note that \(\sum_{i_k} u_{i_k} \prod_{q: \pi(q) = k} \braket{w_{i_k}, v_{j_{\pi'(q)}}} \prec d^{-b(k)/2}\), where \(b(k)\) denotes the size of \(k\)-th block of \(\pi\). Since \(\sum_{k} b(k) = p\), we bound the total expression by \(d^{\Card{\pi'} - \Card{\pi'}/2 - p/2} = d^{\frac{1}{2}(\Card{\pi'} - p)}\).
\end{proof}
Since we assume that \(f_1, g_1\) are odd, terms with \(\Card{\pi} = p - 1\) are equal to zero, and same with \(\Card{\pi'} = p - 1\). When \(\min(\Card{\pi}, \Card{\pi'}) \leq p - 2\), the previous lemma implies \(O(d^{-1})\) total contribution. Therefore, the only case left is with \(\pi = \pi' = \{\{1\}, \{2\}, \ldots, \{p\}\}\). However, in this case it is easy to see that the final contribution is \(O(d^{-p/2})\).

Next, note that perturbation analysis implies that \(\E f_2'(u^\top f_1(W^1 x)) = \E f_2'(N_2) + O(d^{-1/2})\), same for \(g_2\). Finally, using that \(\norm{\Phi_1 - \Phi_1^{\mathrm{lin}}}_F \prec 1,\) we obtain that
\begin{equation}
    \abs{\E f_2(u^{\top} f_1(W^1x)) g_2(z^{\top} g_1(V^1x)) - (\E f_2' (N_2)) (\E g_2' (\tilde N_2))  u^\top \Phi_1^{\mathrm{lin}} z} \prec d^{-1},
\end{equation}
which finishes the proof.
% By computations similar to previous case, we can show that \(R = O(d^{-1})\).
% Also, by weak correlation between the layers (also see previous case), we can show that
% \begin{equation}
%     \begin{aligned}
%         &\braket{\E D^p f_2(u^{\top} f_1(Wx)), \E D^p g_2(z^{\top} g_1(Vx))} \\
%     &\quad = 
%     \sum_{k, j} \braket{w_k, v_j}^p u_k z_j \E \left[f_2' (u^\top f_1(Wx)) f_1^{(p)}(w_k^\top x) \right] \E \left[g_2' (z^\top g_1(Vx)) g_1^{(p)}(v_j^\top x) \right] + O(d^{-1}) \\
%     &\quad = \sum_{k, j} \braket{w_k, v_j}^p u_k z_j \E \left[f_2' (u^\top f_1(Wx))\right] \E \left[f_1^{(p)}(w_k^\top x) \right] 
%     \E \left[g_2' (z^\top g_1(Vx))\right] \E \left[g_1^{(p)}(v_j^\top x) \right] + O(d^{-1})
%     \end{aligned}
% \end{equation}
% After reverting chaos expansion, we obtain that 
% \begin{equation}
%     \E f_2(u^{\top} f_1(Wx)) g_2(z^{\top} g_1(Vx)) = \E \left[f_2' (u^\top f_1(Wx))\right] \E \left[g_2' (z^\top g_1(Vx))\right] u^\top \E \left[f_1(W^\top x)g_1(V^\top x)\right]z + O(d^{-1}).
% \end{equation}
% \begin{lemma}
% \label{lem:off_diag_large_p}
%     For fixed \(p \geq 2\), we have that 
%     \begin{equation}
%         Q \coloneqq \sum_{k \neq j} \braket{w_k, v_j}^p u_k z_j  = O(d^{-1})
%     \end{equation}
% \end{lemma}
% \begin{proof}
%     Indeed, due to independence of \(u\) and \(v\), and since they are mean-zero, we obtain that \(\E_{u,v} Q = 0\) and for the variance
%     \begin{equation}
%         \E Q^2 = \E \sum_{k \neq j}\sum_{k' \neq j'} \braket{w_k, v_j}^p \braket{w_{k'}, v_{j'}}^p u_k z_j u_{k'} z_{j'} = O(d^{-p}),
%     \end{equation}
%     where we used the fact that the only non-zero contribution comes from the pairing between \(k, k', j, j'\).
%     Since \(p \geq 2\), we obtain that with high probability, \(Q = O(d^{-1})\).
% \end{proof}
% Lemma~\cref{lem:off_diag_large_p} implies that
% \begin{equation}
%     \begin{aligned}
%         &\E f_2(u^{\top} f_1(Wx)) g_2(z^{\top} g_1(Vx)) \\
%         &\quad = \sum_{k} \E f_1(w_k^\top x) g_1(v_k^\top x) + \sum_{k \neq j} 
%     \end{aligned}
% \end{equation}
% Therefore, from chaos decomposition, we obtain that 
% \begin{equation}
%     \E f_2(u^{\top} f_1(Wx)) g_2(z^{\top} g_1(Vx)) = 
%     \sum_{k, j} u_k z_j \E f_1(w_k^\top x) g_1(v_j^\top x) + O(d^{-1}).
% \end{equation}
% We can split the 
% \dd{should be doable with straightforward estimates}
% For $g$ we have the previous Wiener chaos expansion
% \begin{equation}
%     g(V x)_i = g(v_i^\top x)= \sum_p \frac{\E g^{(p)}(v_i^\top x)}{p!} I_p( v_i^{\otimes p} )
% \end{equation}
% while for $f_2(W^2 f_1(W^1 x))$ using~\ref{cor:wiener_chaos_general} and~\cref{lem:der_recursion}, we have the more complicated expansion
% \begin{equation}
%     \begin{split}
%         f_2(W^2 f_1(W^1 x))_j &= \sum_p \frac{1}{p!}\sum_{\pi \vdash [p]} \E f_2^{(\abs{\pi})}((w^2_j)^\top f_1(W^1 x))  \,I_p\left(\wt\bigotimes_{B\in \pi}\Bigl(\sum_k W^2_{jk} f_1^{(\abs{B})}((w^1_k)^\top x) (w^1_k)^{\otimes \abs{B}}\Bigr)\right).
%     \end{split}
% \end{equation}
% Thus we obtain
% \begin{equation}
%     \E g(V x)_i f_2(W^2 f_1(W^1 x))_j = \sum_p \frac{1}{p!} [\E g^{(p)}(v_i^\top x)] \sum_\pi \E f_2^{(\abs{\pi})}((w^2_j)^\top f_1(W^1 x)) \prod_{B\in \pi}\Bigl(\sum_k W^2_{jk} f_1^{(\abs{B})}((w^1_k)^\top x) (v_i^\top w^1_k)^{\abs{B}}\Bigr).
% \end{equation}
% \begin{lemma}
%     Let \(F = v^{\top} f(Wx)\). Then,
%     \begin{equation}
%         F = \sum_{p} \frac{1}{p!}I_p\left(\sum_i v_i (\E f^{(p)}(w_i^{\top} x)) w_i^{\otimes p}\right).
%     \end{equation}
% \end{lemma}

% Note that the arguments of $f_2$ and $f_1$ are only weakly correlated, so that we get
% \begin{equation}
%     \E g(V x)_i f_2(W^2 f_1(W^1 x))_j \approx \sum_p \frac{1}{p!} g^{(p)}_i \sum_\pi f^{\abs{\pi}}_{2,j} \prod_{B\in \pi}\Bigl(\sum_k W^2_{jk} f_{1,k}^{\abs{B}} (W^1 V^\top)_{ki}^{\abs{B}}\Bigr).
% \end{equation}
% where
% \begin{equation}
%     g^{(p)}_i\coloneqq\E g^{(p)}(v_i^\top x),\quad f_{2,j}^{q}\coloneqq\E f_2^{(q)}((w^2_j)^\top f_1(W^1 x)), \quad f_{1, k}^{q} \coloneqq \E f_{1}^{(q)}((w^1_k)^\top x).
% \end{equation}

% We further obtain
% \begin{equation}
%     \E g(V x)_i f_2(W^2 f_1(W^1 x))_j \approx \sum_p \frac{1}{p!} g^{(p)}_i \sum_\pi f^{\abs{\pi}}_{2,j} \bigodot_{B\in \pi}\Bigl((V (W^1)^{\top})^{\odot \abs{B}}\mathrm{diag}(f_{1}^{\abs{B}}) (W^2)^{\top}\Bigr)_{ij}.
% \end{equation}
% Let's analyze partition \(\pi_s = \{\{1\}, \ldots, \{p\}\}\) into singletons.
% We have a term
% \begin{equation}
%     f_{2,j}^p \cdot \left(V (W^1)^{\top} \mathrm{diag}(f_1^{\abs{B}}) (W^2)^{\top}\right)^p_{ij}.
% \end{equation}
% For all other partitions, we have that off-diagonal terms of \(V (W^1)^{\top}\) have sub-leading \(O(d^{-1})\) order and we can approximate.
% % \begin{equation}
% %     \E g(V x)_i f_2(W^2 f_1(W^1 x))_j \approx 
% %     \sum_p \frac{1}{p!} g^{(p)}_i \sum_\pi f^{\abs{\pi}}_{2,j} \sum_{k_1, \ldots, k_{\abs{\pi}}} \prod_{q=1}^{\abs{\pi}} f_{1, k_q}^{b(q)} W^2_{j k_q} (W^1 V^{\top})_{k_q i}^{b(q)}
% % \end{equation}

% % \paragraph{off-diagonal entries}

% % If $\abs{v_i^\top w_k^1}\lesssim n^{-1/2}$ for all $i,k$ then to leading order only the partition into singletons contributes and we obtain
% % \begin{equation}
% %     \E g(V x)_i f_2(W^2 f_1(W^1 x))_j \approx \sum_p \frac{1}{p!} g^{(p)}_i f_{2,j}^{(p)} \Bigl( W^2 f_{1}^{'} W^1 V^\top \Bigr)_{ji}^p
% % \end{equation}
% % where we expect only the $p=1$ term to contribute.
% % \paragraph{diagonal entries}


% \subsection{Trying another approach}

\subsection{Proof of~\Cref{lem:diag}}
For convenience we restate several concentration results that follow from~\Cref{corr rainbow}.
\begin{lemma}
\label{lem:conc_inequalities}
    Let \(w_1, \ldots, w_d\) be a collection of independent random vectors, such that for all \(\norm{w_i}_{\psi_2} = O(d^{-1/2})\) and \(\norm{w_i} = O(1)\) for all \(i \in [d]\).
    Then 
    \begin{enumerate}[label=(\roman*)]
        % \item For any \(i \neq j\), \(\braket{w_i, w_j} \prec d^{-1/2}\),
        \item \(\sum_i \braket{w_1, w_i} \prec 1\),
        \item \(\sum_{ij} \braket{w_1, w_i} \braket{w_1, w_j} \prec 1\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    Let \(X = \sum_{i \geq 2} \braket{w_1, w_i}\). Note that \(\norm{X}_{\psi_2} = O(1)\), which, together with \(\norm{w_1} = O(1)\) implies (i).

    For \((ii)\), note that \(\sum_{ij} \braket{w_1, w_i} \braket{w_1, w_j} = \left(\sum_i \braket{w_1, w_i}\right)^2 \prec 1\) using \((i)\).
\end{proof}
Let \(F_1 = u^\top f_1(W^1 x)\) and \(F_2 = z^\top g_1(V^1 x)\), where \(u, z\) may be correlated. 
For simplicity we omit indices in \(f_1, g_1, W^1\), and \(V^1\).
Using Wiener chaos expansion, we obtain
\begin{equation}
    F_1 = u^\top f(Wx) = \sum_{p \geq 1} I_p\left(\frac{\E D^p F_1}{p!}\right) = \sum_{p \text{ odd}} I_p \left(\sum_i \frac{u_i w_i^{\otimes p} \E f^{(p)} (w_i^\top x)}{p!}\right),
\end{equation}
where the last equality uses the fact that \(\E f^{(p)} (w_i^\top x) = 0\) for even \(p\).
 Similarly, we can write
\begin{equation}
    F_2 = z^\top g(Vx) = \sum_{p \text{ odd}} I_p \left(\sum_i \frac{z_i v_i^{\otimes p} \E g^{(p)} (v_i^\top x)}{p!}\right),
\end{equation}
and denote \(f_i^p \coloneqq \E f^{(p)} (w_i^\top x), g_i^p \coloneqq \E g^{(p)} (v_i^\top x)\). 
Next, we compute
\begin{equation}
    \begin{aligned}
    D F_1 &= \sum_{\text{odd } p} p I_{p -1} \left(\sum_i \frac{u_i w_i^{\otimes p} f_i^p}{p!} \right) \quad \text{and} \\
    -D L^{-1} F_2 &= \sum_{\text{odd } q} I_{q -1} \left(\sum_i \frac{z_i v_i^{\otimes q} g_i^q}{q!} \right)
    \end{aligned}
\end{equation}
\begin{lemma}
    \begin{equation}
        \E (\braket{D F_1, -D L^{-1} F_2} - \E F_1 F_2)^2 = O(d^{-1}).
    \end{equation}
\end{lemma}
\begin{proof}
    Since \(I_{p - 1} (w_i^{\otimes p}) = I_{p - 1}(w_i^{\otimes p - 1}) w_i\), we can write
    \begin{equation}
        \braket{D F_1, -DL^{-1} F_2} = \sum_{\substack{\text{odd } p \\ {\text{odd } q}}} c_{pq} \sum_{i, j} \braket{w_i, v_j} u_i z_j f_i^p g_j^q I_{p - 1}(w_i^{\otimes p - 1}) I_{q - 1}(w_j^{\otimes q - 1}).
    \end{equation}
    Using (\cite{nourdin2012normal}, Theorem 2.7.10), we obtain
    \begin{equation}
    \begin{aligned}
        & I_{p - 1}(w_i^{\otimes p - 1} I_{q - 1}(w_j^\otimes q - 1) \\
        & \quad = \sum_{r = 0}^{p \land q - 1} \braket{w_i, v_j}^r c_{rpq} I_{p + q - 2(r + 1)} (w_i^{\otimes p - 1 - r} \wt \otimes v_j^{\otimes q - 1 - r}) \\
        & \quad = \sum_{\substack{s = |p - q| \\ 2 \text{ divides } (s - |p - q|)}}^{p + q - 2} c'_{spq} \braket{w_i, v_j} ^{(p + q - 2 - s) / 2} I_s(w_i^{\otimes p - q + s} \wt \otimes v_j^{q - p + s}).
    \end{aligned}
    \end{equation}

Therefore, for 
\begin{equation}
    \begin{aligned} 
    J_s \coloneqq \Big\{(p, q) \in \N^2 \text{, such that }\abs{p - q} &\le s, 2 \text{ divides } \bigl(s - \abs{p - q}\bigr), \\
     \min(p, q) &\ge 1 + \frac{s - |p - q|}{2}\Big\},
    \end{aligned}
\end{equation} we obtain
\begin{equation}
    \begin{aligned}
    &\braket{DF_1, -DL^{-1}F_2} \\
    &\quad = \sum_{\substack{s \geq 0 \\ p,q \in J_s}} \tilde c_{r,p,q}\sum_{i, j}\braket{w_i, v_j}^{(p+q - s)/2} u_i z_j f^p_i f^q_j \\
    &\qquad \qquad \qquad \cdot I_s(w_i^{\otimes (s + p - q) / 2} \wt \otimes v_j^{\otimes (s + q - p) / 2}).
    \end{aligned}
\end{equation}
The term \(s = 0\) corresponds to \(\E F_1 F_2\). Since \(p\) and \(q\) must be odd in the non-zero terms of the sum, we obtain that \(s\) must be even. For \(a \coloneqq (p+q-s)/2\), the \(s\)-th multiplie integral \(I_s\) can be rewritten as follows:
\begin{equation}
    I_s\left(\sum_{a \geq 1} \sum_{i, j} \braket{w_i, v_j} u_i z_j T_{ij}^s\right),
\end{equation}
where \(T_{ij}^s\) is a \(s\)-dimensional tensor, consisting of a sum of inner products of \(w_i\) and \(v_j\), also containing combinatorial terms, and products of expectations of derivatives of \(f, g\). We can write
\begin{equation}
    \E \left(\braket{D F_1, -DL^{-1} F_2} - \E F_1 F_2\right)^2 = \sum_{s \geq 2} \E I_s\left(\sum_{a \geq 1} \sum_{i,j} \braket{w_i, v_j} u_i z_j T_{ij}^s\right)^2.
\end{equation}
Fix \(s \geq 2\) and observe that 
\begin{equation}
\label{eq:fixed_s_expansion}
\begin{aligned}
    &\E I_s \left(\sum_{a \geq 1} \sum_{i,j} \braket{w_i, v_j} u_i z_j T_{ij}^s\right)^2 \\
    &\quad = 
    \sum_{a,a' \geq 1} \sum_{\substack{i,j\\i', j'}} \braket{w_i, v_j}^a \braket{w_{i'}, v_{j'}}^{a'} u_i u_{i'} z_j z_{j'} \braket{T_{ij}^s, T_{i'j'}^s},
\end{aligned}
\end{equation}
and note that for some constant \(C > 0\) (depending on combinatorial terms, and products of expectations of derivatives of \(f\))
\(\braket{T_{ij}^s, T_{i' j'}^s}\) can be upper bounded by
\begin{equation}
    \braket{T_{ij}^s, T_{i'j'}^s} \leq C(\braket{w_i, w_{i'}} + \braket{w_i, v_{j'}} + \braket{v_j, w_{i'}} + \braket{v_j, v_{j'}})^s.
\end{equation}
We analyze each term of the summand in~\Cref{eq:fixed_s_expansion} depending on \(a, a', i, i', j, j'\).
Let \(N = \lvert\{i, i', j, j'\}\rvert\), the number of distinct indices among \(i, i', j, j'\).
Since entries of \(u\) and \(z\) are \(O(d^{-1/2})\), we get that in total the term \(u_i u_{i'} z_j z_{j'}\) contributes \(O(d^{-2})\).

\paragraph{Case \(N = 1\)}
Here, since there are only \(d\) such terms in total, we immediately obtain an \(O(d^{-1})\) upper bound.
\paragraph{Case \(N = 2\)}
There are \(O(d^2)\) such terms. It must be that either (i) \(i \neq j\) or (ii) \(i' \neq j'\) or (iii) both \(i = j\) and \(i' = j'\).
In the latter case, we obtain bound \(O(d^{-1})\) since \(s \geq 2\), and thus \(\braket{T_{ij}^s, T_{i'j'}^s} = O(d^{-1})\).
Otherwise, without loss of generality, assume that \(i = i' = j'\) and \(a = 1\). Here,~\Cref{lem:conc_inequalities} (i) implies that the summand is \(\prec 1/d\).
\paragraph{Case \(N = 3\)}
If \(i = j\), note that \(\braket{T_{ij}^s, T_{i'j'}^s} \prec d^{-1}\) and we need to show that \(\sum_{i \neq i' \neq j'} \braket{w_{i'}, v_{j'}}^a \prec d^2\).
When \(a \geq 2\) this follows from asymptotic orthogonality (\(\braket{w_{i'}, v_{j'}} \prec d^{-1/2}\) for \(i' \neq j'\), see~\Cref{corr rainbow}), and otherwise from~\Cref{lem:conc_inequalities} (i).
If \(i = i'\), we need to show that 
\begin{equation}
    \sum_{i j j'} \braket{w_i, v_j}^a \braket{w_{i'}, v_{j'}}^{a'} d^{-2} \prec \frac{1}{d}.
\end{equation}
When \(a \geq 2\) and \(a' \geq 2\), this follows from asymptotic orthogonality.
When \(a = 1\) and \(a' \geq 2\) (or vice versa), this follows from~\Cref{lem:conc_inequalities} (i).
Finally, when \(a = 1\) and \(a' = 1\), this follows from~\Cref{lem:conc_inequalities} (ii).
The remaining cases are identical to the covered ones.
\paragraph{Case \(N = 4\)}
When \(a \geq 2\) and \(a' \geq 2\), the result follows trivially.
When \(a = 1\) and \(a' \geq 2\), the result follows from~\Cref{lem:conc_inequalities} (i).
When \(a = a' = 1\), the result follows again from~\Cref{lem:conc_inequalities} (i) and noticing that 
\begin{equation}
    \frac{1}{d^2} \sum_{\substack{i \neq j \\ i' \neq j'}} \braket{w_i, v_j} \braket{w_{i'}, v_{j'}} = \left(\frac{1}{d} \sum_{i \neq j} \braket{w_i, v_j}\right)^2 \prec 1.
\end{equation}
\end{proof}
Next, using~\Cref{thm:multi-clt} for \(h(F_1, F_2) = f_2(F_1)g_2(F_2)\), we obtain that 
\begin{equation}
    \abs{\E f_2(u^\top f_1(W^1 x)) g_2(z^\top g_1(V^1 x)) - \E f_2(G_1) g_2(G_2)} \prec d^{-1/2},
\end{equation}
where \((G_1, G_2)\) is a jointly Gaussian random vector:
    \begin{equation}
        (G_1, G_2) \sim \cN \left(0,  \begin{pmatrix} \Tr \left[u u^\top \Omega_1 \right] & \Tr \left[u z^\top \Phi_1 \right]\\ \Tr \left[u z^\top \Phi_1 \right] & \Tr \left[z z^\top \Psi_1 \right] \end{pmatrix} \right).
\end{equation}
Finally, using that \(\norm{\Omega_1 - \Omega_1^{\mathrm{lin}}}_F \prec 1\) (same for \(\Phi_1, \Psi_1\)) and perturbation analysis, we obtain that 
\begin{equation}
    \abs{\E f_2(u^\top f_1(W^1 x)) g_2(z^\top g_1(V^1 x)) - \E f_2(N_1) g_2(N_2)} \prec d^{-1/2}.
\end{equation}

\subsection{Extending to \(L \geq 3\)}
A natural question is to ask whether the same technique can be applied for a deeper networks. One possible direction is to apply~\Cref{thm:multi-clt} for \(d\)-dimensional vector \((F_1, \ldots F_d) \coloneqq (u_1^\top f_1(W^1 x), \ldots, u_d^\top f_1(W^1 x))\), to approximate it by a Gaussian random vector \((N_1, \ldots, N_d)\). Then, for example, the diagonal entries of \(\Omega_3\) can be written as \(h(F_1, \ldots, F_d) = f_3(\sum_{k} u_k f_2(F_k))^2\). If it is possible to derive that \(f_3(\sum_{k} u_k f_2(F_k))^2 = f_3(\sum_k u_k f_2(N_k))\), then the problem is reduced to the 2 layered case, which can be treated as before.

However, it seems hard to apply~\Cref{thm:multi-clt} to the \(d\)-dimensional vector, since this requires a much more careful error analysis. Recall that we only applied~\Cref{thm:multi-clt} to \(2\)-dimensional vectors. We leave the extension to \(L, \wt L \geq 3\) as an interesting open question.