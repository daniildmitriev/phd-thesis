\chapter{Appendix of Greedy}
\section{Auxiliary lemmas}\label{appendix:aux_lemmas}
%
\begin{lemma}\label{lemma:chernoff}(Chernoff Bound - upper tail)
Let $X_1, ..., X_n$ be independent random variables taking values in $\set{0, 1}$, $X$ denote their sum and $\mu = \E X$. Then for any $\delta > 0$,
    \begin{align*}
        \Pr\br{X \geq (1+\delta)\mu}\leq e^{-\delta^2 \mu / (2+\delta)}.
    \end{align*}
\end{lemma}


\noindent
In order to deal with concentration of $\dmax$ around its expectation, we state the following useful result on tensorization of variance. We introduce notation \(\text{Var}_i\) and \(\E_i\), where subscript $i$ indicates conditioning on each component of an underlying random vector, except for the $i$-th one.
\begin{lemma}[Theorem 2.3, \cite{van2014probability}]\label{lemma:tensorization_var}
Let $X_1,...,X_n$ be independent random variables and for each function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, define 
\[\text{Var}_i f(x_1,...,x_n) := \text{Var}\left(x_1,...,x_{i-1},X_i,x_{i+1},...,x_n\right).\]
Then, there holds that 
\[\text{Var} \left(f\br{X_1,...,X_n}\right ) \leq \E \sum_{i = 1}^n \text{Var}_i f\br{X_1,...,X_n}\]
\end{lemma}


\begin{lemma}[Concentration for $\dmax$]\label{lemma:d_max_concentration}
    Let $X_1, \ldots, X_n  \overset{\underset{\mathrm{iid}}{}}{\sim} Bin(m,p)$. Then, for any $t > 0$, 
    \begin{align*}
        \Pr\br{\left |\dmax-\E \dmax\right|> t}\leq \frac{mp}{t^2}.
    \end{align*}
\end{lemma}
\begin{remark}
    Note that in all regimes of $m, p$ satisfying Assumption \ref{asmpt:A_np_m}, choosing $t \sim {\E \dmax}$ is sufficient to deduce from the previous lemma that $\dmax\sim \edmax$ w.h.p..
\end{remark}
\begin{proof}
    Proceeding by Chebyschev's inequality, it suffices to show that $\text{Var}(\dmax\leq mp$. By Lemma~\ref{lemma:tensorization_var}, we have that 
    \begin{align*}
        \text{Var}(\dmax) & \leq \E \sum_{i = 1}^n \E_i\br{ \dmax - \E_i \dmax}^2\\
        & = \E \sum_{i = 1}^n \E_i\left[\br{ \dmax - \E_i \dmax}^2\mid \dmax = X_i \right]\Pr \br{\dmax = X_i} \\
        & +  \E \sum_{i = 1}^n \E_i\left[\br{ \dmax - \E_i \dmax}^2\mid \dmax \neq X_i \right]\Pr \br{\dmax \neq X_i} \\
        & = \frac1n\E \sum_{i = 1}^n \text{Var}X_i \\
        & \leq mp,
    \end{align*}
which is as required.
\end{proof}

\begin{lemma}[Asymptotic expression for binomial probability mass function]
\label{lemma:binom_lb}
%
~\\~
    Let \(a \equiv a(n)\) and \(b \equiv b(n)\) be such that
    \begin{enumerate}
        \item \(1 \ll b \ll \sqrt{a}\),
        \item \(p \ll 1\). 
    \end{enumerate}
    If \(b \geq C ap\) for \(C > 1\), then 
    \begin{equation}
    \label{eq:binom_lb_small_b}
        \log \Pr(\mathrm{Bin}(\ceil a, p) = \ceil b) \geq -\left(b \log \frac{b}{a p} - b + ap\right) (1 + o(1)),
    \end{equation}
    If also \(b \gg ap\), we have that 
    \begin{equation}
    \label{eq:binom_lb_large_b}
        \log \Pr(\mathrm{Bin}(\ceil a, p) = \ceil b) \geq -\left(b \log \frac{b}{a p}\right) (1 + o(1)),
    \end{equation}
Furthermore, all bounds remain valid upon replacing \(\ceil a\) to \(\floor a\).
\end{lemma}
\begin{proof}
    \begin{align*}
\Pr(\mathrm{Bin}(\ceil a, p) = \lceil b \rceil) &= {\ceil a \choose \lceil b \rceil} p^{\lceil b \rceil}(1 - p)^{\ceil a - \lceil b \rceil} \\
&\overset{(i)}{\geq} \frac{(\ceil a p)^{  \ceil{b}  }}{4 (  \ceil{b}  )!} (1 - p)^{\ceil a -   \ceil{b}  } \\
&\overset{(ii)}{\geq} \frac{1}{\ceil b} \left(\frac{\ceil a e p}{  \ceil{b}  } \right)^{  \ceil{b}  } (1 - p)^{\ceil a -   \ceil{b}  } \\
&\overset{(iii)}{\geq} \frac{1}{\ceil b} \left(\frac{\ceil a e p}{  \ceil{b}  } \right)^{  \ceil{b}  } e^{-\ceil a p} (1 - p)^{\ceil a p - \ceil{b} },
% &\geq \frac{1}{4\Pri \sqrt{ \tau \Delta  }} \left(\frac{e \tilde{m}p}{  \tau \Delta  } \right)^{  \ceil{\tau \Delta}  } e^{-\tilde{m}p} (1 - p)^{\tilde{m}p -   \ceil{\tau \Delta}  },
\end{align*}
where $(i)$ is due to ${n \choose k} \geq \frac{n^k}{4k!}$ for $0 \leq k \leq \sqrt{n}$, $(ii)$ is due to $n! \leq \frac{n}{4}(n/e)^n$ for $n$ large enough, and $(iii)$ is due to $(1 + x/n)^n \geq e^x (1 - x^2/n)$ for $|x|\leq n$.
After taking the logarithm, we get 
\begin{equation*}
    \begin{aligned}
    &\log \Pr(\mathrm{Bin}(\ceil a, p) = \lceil b \rceil) \\
    &\quad \geq - \left(\ceil b \log \frac{\ceil b}{\ceil a  p} - \ceil b + \ceil a p +  \log \ceil{b} - (\ceil a p - \ceil b) \log (1 - p) \right).
    \end{aligned}
\end{equation*}
If \(b \geq C \ceil a p\) for \(C > 1\), we have that 
\begin{equation*}
    b \log \frac{b}{ap} - b + ap \geq \gamma b \gg 1, \qquad \text{for } \gamma := \frac{1}{C} + \log C - 1 > 0.
\end{equation*}
Since \(b \gg 1\), we have
\begin{equation*}
\frac{\ceil b \log \frac{\ceil b}{\ceil a p} - \ceil b + \ceil a p}{ b \log \frac{b}{ap} - b + ap} = 1 + o(1).
\end{equation*}
Now, since also \(\log \ceil b \ll b\) and \((\ceil a p - \ceil b) \log (1 - p) \ll b\) for \(p \ll 1\), we have that
\begin{equation*}
    \log \Pr(\mathrm{Bin}(\ceil a, p) = \ceil b) \geq - \left(b \log \frac{b}{ap} - b + ap\right)(1 + o(1)).
\end{equation*}
If additionally \(b \gg ap\), then 
\begin{equation*}
    \frac{b \log \frac{b}{ap} - b + ap}{b \log \frac{b}{ap}} = 1 + o(1),
\end{equation*}
and, finally,
\begin{equation*}
    \log \Pr(\mathrm{Bin}(\ceil a, p) = \lceil b \rceil) \geq - \left(b \log \frac{b}{ap}\right)(1 + o(1)).
\end{equation*}
Under our assumptions, \(1 \ll b \ll \sqrt{a}\), the same bounds hold for \(\log \Pr(\mathrm{Bin}(\floor a, p) = \floor b)\).
\end{proof}

\begin{lemma} [Binomial Monotonicity] \label{lemma:binomial_monotonicity}
Let $S_m \sim \mathrm{Bin}(m, p)$. Then for $r \geq mp$, we have that $\Pr(S_m = r + 1) \leq \Pr(S_m = r)$ and \(\Pr(S_{m-1} = r) \leq \Pr(S_{m} = r)\).
\end{lemma}
%
\begin{proof}
The proof follows a similar argument as that presented in \cite{bunke_feller_1969}.  
\begin{align*}
\frac{\Pr(S_m = r + 1)}{\Pr(S_m = r)} &= \frac{ {m \choose r+1} p^{r+1} (1 - p)^{m - r - 1} }{ {m \choose r} p^r (1 - p)^{m - r} } \\
&= \frac{ \frac{m!}{(r + 1)! (m - r - 1)!} p^{r + 1} (1-p)^{m - r - 1}}{ \frac{m!}{r! (m - r)!} p^r (1-p)^{m - r}} \\
&= \frac{(m - r) p }{(r + 1) (1-p)} \leq 1. 
\end{align*}
Similar arguments show that \(\Pr(S_{m-1} = r) \leq \Pr(S_{m} = r)\).
\end{proof}

\section{Main tool for the case \(mp \lesssim \log n\) and Proof of Lemma~\ref{lemma:mp<logn_conquering_logn}}\label{app:geom_alg}

\begin{lemma}
\label{lemma:geom_alg}
    If \(mp \lesssim \log n\), then, for any \(\varepsilon > 0\), there exist constants \(\tau > 0\) and \(1 < \alpha < \beta\), such that, for \(k \lesssim \log n\) and for any \(\tilde m\), satisfying \(\beta^{-k-1}m \leq \tilde m \leq \beta^{-k} m\), for all \(n\) large enough,
    \begin{equation*}
        \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha / \beta)^{k} \tau \edmax\bigr\rceil \biggr) \geq n^{-\varepsilon}.
    \end{equation*}
\end{lemma}
\begin{proof}
    The proof is essentially a careful application of Lemma~\ref{lemma:binom_lb}.
    Let \(\tau, \alpha, \beta\) be constants to be fixed later and \(\tilde m = \floor{\beta^{-k-1}m}\). Depending on whether we have \(mp \ll \log n\) or \(mp \sim \log n\), different terms will dominate the asymptotic expression from Lemma~\ref{lemma:binom_lb}.

    \noindent
    We start with the case \(mp \ll \log n\). From Lemma~\ref{lemma:maximal_ineq}, this implies that \(mp \ll \edmax \ll \log n\). Here we can fix \(\alpha \equiv 2\) and \(\beta \equiv 3\). Applying~(\ref{eq:binom_lb_large_b}) for \(a = 3^{-k-1}m\) and \(b = (2/3)^k \tau \edmax\), we have:
    \begin{equation}
        \begin{aligned}
        &\log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (2/3)^k \tau \edmax\bigr\rceil \biggr) \\
        &\quad \geq - (2/3)^k \tau \edmax \log \left(\frac{2^k 3 \tau \edmax}{ mp}\right) (1 + o(1))
        \end{aligned}
    \end{equation}
    Recall that our goal is to show \(\log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (2/3)^k \tau \edmax\bigr\rceil \biggr) \geq -\varepsilon \log n\). We first show that there exists \(\tau > 0\) satisfying the following two inequalities:
    \begin{equation}
    \begin{aligned}
        &(i) \qquad (2/3)^k \tau (\log 3 + k \log 2 ) \frac{\edmax}{\log n} &\leq \quad \frac{\varepsilon}{4}, \\
        &(ii) \qquad (2/3)^k \tau \frac{\edmax}{\log n} \log \left(\frac{\edmax}{mp}\right) &\leq \quad \frac{\varepsilon}{4}.
    \end{aligned}
    \end{equation}
    Indeed, since \(\edmax \ll \log n\) and \(k \ll (3/2)^k\), inequality \((i)\) will be satisfied for any \(\tau > 0\) for \(n\) large enough. For \((ii)\) we need to use explicit bound for \(\edmax\), in particular from Lemma~\ref{lemma:maximal_ineq} we know that there exists \(C > 0\), such that \(\edmax \leq C \log n / (\log \log n - \log mp)\) for \(n\) large enough. Plugging this into \((ii)\), we get for \(k = 0\),
    \begin{equation}
        \begin{aligned}
        &\tau \frac{\mathbb{E}\dmax}{\log n}\log\left(\frac{\mathbb{E}\dmax}{mp}\right) \\
        &\quad \leq \frac{\tau C (\log C + \log \log n - \log (\log \log n - \log mp) - \log mp)}{\log \log n - \log mp} \\
        &\quad = \tau C + o(1).
        \end{aligned}
    \end{equation}
    For \(\tau = \varepsilon / (8C)\), \((ii)\) holds for \(k = 0\) for \(n\) large enough. By increasing \(k\) we only decrease left hand side of \((ii)\), therefore, the same value of \(\tau\) works for any \(k \geq 0\). 

    \noindent
    Finally, by adding \((i)\) and \((ii)\) we showed that, for \(n\) large enough, 
    \begin{equation*}
    \log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha/2)^k \tau \edmax\bigr\rceil \biggr) \geq - \frac{\varepsilon}{2}\log n (1 + o(1)) > -\varepsilon \log n,
    \end{equation*}
    which finishes the proof for the case \(mp \ll \log n\).

    \noindent
    Now we focus on the case \(mp \sim \log n\). Here we apply~(\ref{eq:binom_lb_small_b}) for the values \(a = {\beta}^{-k-1}m\) and \(b = (\alpha / \beta)^k \tau \edmax\) keeping in mind the condition \(b \geq Cap\) with \(C > 1\). 
    We have
    \begin{equation*}
    \begin{aligned}
         &\log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha / \beta)^k \tau \edmax\bigr\rceil \biggr) \\
         &\quad \geq - \Bigg((\alpha / \beta)^k \tau \edmax \log \Big(\frac{\beta \alpha^k \tau \edmax}{ mp}\Big) \\ 
         &\qquad \qquad  -(\alpha / \beta)^k \tau \edmax + \beta^{-k-1}mp\Bigg)(1 + o(1)) %\\
    \end{aligned}
    \end{equation*}
    We pick \(\tau = \gamma mp / \edmax\), for some constant \(\gamma > 1\) to be specified later. Note that this way condition for applying~(\ref{eq:binom_lb_small_b}), \(\frac{b}{ap} \geq C > 1\), is satisfied since \(\frac{b}{ap} \geq \frac{\tau \edmax}{mp} = \gamma > 1\). This simplifies the latter expression to the following:
       \begin{equation*}
    \begin{aligned}
         &\log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha / \beta)^k \gamma mp\bigr\rceil \biggr) \\
         &\quad \geq - mp\left((\alpha / \beta)^k \gamma \log \bigl(\beta \gamma \alpha^k\bigr) - (\alpha / \beta)^k \gamma + \beta^{-k-1}\right)(1 + o(1)) %\\
    \end{aligned}
    \end{equation*}
Since in this regime we have \(mp \leq D \log n\) for some \(D > 0\), for \(n\) large enough, it is enough to show 
\begin{equation*}
   (\alpha / \beta)^k \gamma \log \bigl(\beta \gamma \alpha^k\bigr) - (\alpha / \beta)^k \gamma + \beta^{-k-1} \leq \varepsilon / (2D).
\end{equation*}
We first show that there exist constants \(1 < \alpha < \beta\) and \(\gamma > 1\), depending on \(\varepsilon\) and \(D\), satisfying the following two inequalities for any \(k \geq 0\):
\begin{equation*}
    \begin{aligned}
        &(i) \qquad (\alpha/\beta)^k \left(\gamma \log \beta \gamma - \gamma + \frac{1}{\alpha^k \beta}\right) &\leq \quad \frac{\varepsilon}{4D}, \\
        &(ii) \qquad (\alpha/\beta)^k k \log \alpha & \leq \quad \frac{\varepsilon}{4D}.
    \end{aligned}
\end{equation*}
Note that left hand side of \((i)\) decreases as \(k\) increases, therefore, it is enough to look at \(k = 0\). We need to show that there exist \(\beta, \gamma > 1\), depending on \(\varepsilon, D\) such that
\begin{equation*}
    f(\beta, \gamma) := \gamma \log \beta \gamma - \gamma + \frac{1}{\beta} \leq \frac{\varepsilon}{4D}.
\end{equation*}
Note that \(\frac{\partial f}{\partial \beta} = \gamma / \beta - 1 / \beta^2 > 0\) and \(\frac{\partial f}{\partial \gamma} = \log \beta \gamma > 0\) as long as \(\beta \gamma > 1\). Since \(f(1, 1) = 0\), we can find \(\beta, \gamma > 1\), close enough to 1, such that \(f(\beta, \gamma) \leq \varepsilon / (4D)\).
We use these values of \(\beta\) and \(\gamma\) (or, equivalently, \(\tau\)). Since \(k \ll (\beta / \alpha)^k\), there exists \(\alpha \in (1, \beta)\), such that \((ii)\) holds. Summing \((i)\) and \((ii)\) shows that, for \(n\) large enough,
\begin{equation*}
\begin{aligned}
    \log \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha / \beta)^k \gamma mp\bigr\rceil \biggr) &\geq - \frac{\varepsilon mp}{2D}(1 + o(1)) \\
    & \geq - \frac{\varepsilon \log n}{2} (1+o(1)) \\
    & \geq -\varepsilon \log n.
\end{aligned}
\end{equation*}
    We proved that for \(mp \lesssim \log n\), for any \(\varepsilon > 0\), for \(n\) large enough, there exists \(\tau, \alpha, \beta\), such that
       \begin{equation*}
        \mathrm{Pr}\biggl(\mathrm{Bin}(\floor {\beta^{-k-1} m}, p) = \lceil(\alpha/\beta)^k \tau \mathbb{E}\dmax\rceil \biggr) \geq n^{-\varepsilon}.
    \end{equation*}
    Since \(\beta^{-k-1}mp < \beta^{-k}mp < \lceil(\alpha/\beta)^k \tau \mathbb{E}\dmax\rceil\), from binomial monotonicity, Lemma~\ref{lemma:binomial_monotonicity}, we have that for any \(\tilde m\) such that \(\beta^{-k - 1} m \leq \tilde m \leq \beta^{-k} m\),
    \begin{equation*}
    \Pr\Bigl(\mathrm{Bin}(\tilde m, p) = \lceil(\alpha / \beta)^k \tau \mathbb{E}\dmax\rceil \Bigr) \geq n^{-\varepsilon}.
    \end{equation*}

\end{proof}
\begin{proof}[Proof of Lemma~\ref{lemma:mp<logn_conquering_logn}]
We follow the argument in Lemma~\ref{lemma:geom_alg} with \(k = 0\) and \(\edmax\) replaced by \(\log n / (\log \log n - \log mp)\). Note that in the proof of Lemma~\ref{lemma:geom_alg}, in the case \(mp \ll \log n\), we only used that \(mp \ll \edmax \ll \log n\) and \(\edmax \leq C \log n / (\log \log n - \log mp)\) for some \(C > 0\). Since both these properties remain true upon replacing \(\edmax\) with \(\log n / (\log \log n - \log mp)\), the proof follows. Since \(\tau = \varepsilon / (8C)\), in the setting of Lemma~\ref{lemma:mp<logn_conquering_logn}, and \(C = 1\) in this argument, we pick \(\tau = \varepsilon / 8\).
\end{proof}
\section{Proof of lemma \ref{lemma:find_one_col_lb}} \label{sec:appendix_proof_lemma_8}
%
We proceed in the proof by first showing that there exists \(\tilde K\), such that \(m - F_{\tilde K} \lesssim \tilde K\), and then, by increasing \(\tilde K\) by a multiplicative factor, we find \(K\) such that \(m - F_K \leq K\).
\subsection*{Case \(mp \lesssim \log n\)} 
From Lemma~\ref{lemma:geom_alg}, there exist constants \(\tau > 0\), \(\alpha, \beta\) with \(1 < \alpha < \beta\), such that, for any \(\tilde m\), satisfying \(\beta^{-k-1}m \leq \tilde m \leq \beta^{-k} m\), for all \(n\) large enough,
    \begin{equation*}
        \Pr\biggl(\mathrm{Bin}(\tilde m, p) = \bigl\lceil (\alpha / \beta)^{k} \tau \edmax\bigr\rceil \biggr) \geq n^{-\varepsilon}.
    \end{equation*}
Recall that in this case \(f_t  = \bigl\lceil (\alpha / \beta)^{k} \tau \edmax\bigr\rceil\), where \(k\) is such that \(\beta^{-k-1}m \leq m - F_{t-1} \leq \beta^{-k} m\) and \(F_t = \sum_{s = 1}^t f_s\). From Lemma~\ref{lemma:geom_alg} we have that \(\Pr(\mathrm{Bin}(m - F_{t-1}, p) = f_t) \geq n^{-\varepsilon}\). Our goal is to prove that there exists \(s \lesssim \vallp \sim m / \edmax\), such that \(m - F_s \lesssim s\). 

\begin{lemma} \label{lemma:one_step_beta}
    Let \(t^{(k)} := \frac{\beta - 1}{\beta\tau} \frac{m}{\edmax} \alpha^{-k}\). 
    \begin{equation*}
        \begin{aligned}
            \text{If} \qquad &m - F_{t-1} \leq \beta^{-k} m  \\
            \text{then} \qquad &m - F_{t+t^{(k)}-1} \leq \beta^{-k-1} m.
        \end{aligned}
    \end{equation*}
    Informally, if after \(t - 1\) steps of \bgreedy, at most \(\beta^{-k} m\) subsets are uncovered, then after \(t + t^{(k)} - 1\) steps, at most \(\beta^{-k-1}m\) subsets remain uncovered.
\end{lemma}
\begin{proof}
    Let \(s \geq t\). As long as \(m - F_{s - 1} > \beta^{-k-1}m\), we will always have \(f_s = \bigl\lceil (\alpha / \beta)^{k} \tau \edmax\bigr\rceil\). We proceed by contradiction. Assume that \(m - F_{t + t^{(k)} - 1} > \beta^{-k-1}m\). This implies that for all \(s \in [t - 1, t + t^{(k)} - 1]\), we have \(f_s = f := \bigl\lceil (\alpha / \beta)^{k} \tau \edmax\bigr\rceil\). Therefore, 
    \begin{equation*}
    F_{t + t^{(k)} - 1} - F_{t - 1} = t^{(k)} f \geq \frac{m(\beta - 1)}{\beta^{k+1}} = \beta^{-k}m - \beta^{-k-1}m,
    \end{equation*}
    and 
    \begin{equation*}
    \begin{aligned}
    m - F_{t + t^{(k)} - 1} &= m - F_{t - 1} - \left(F_{t + t^{(k)} - 1} - F_{t - 1}\right) \\
    & \leq \beta^{-k}m - (\beta^{-k}m - \beta^{-k-1}m) = \beta^{-k-1}m.
    \end{aligned}
    \end{equation*}
    Therefore, we must have \(m - F_{t + t^{(k)} - 1} \leq \beta^{-k-1}m\).
\end{proof}
\noindent
Note that we always have \(\beta^{-1}m \leq m - F_0 = m\).
If we consecutively apply Lemma~\ref{lemma:one_step_beta} starting with \(k = 0\), then, for \(v(k) \coloneqq \sum_{s=0}^k t^{(s)}\) we have \(m - F_{v(k) - 1} \leq \beta^{-k-1} m\). Therefore, for \(k := \frac{\log \edmax}{\log {\beta}}\), we have \(m - F_{v(k) - 1} \leq \frac{m}{\edmax}\). We can bound
\begin{equation*}
v(k) \leq \sum_{s = 0}^{\infty} t^{(k)} = \frac{\beta - 1}{\beta \tau (\alpha - 1)} \frac{m}{\edmax} \sim \frac m \edmax.
\end{equation*}
From Lemma~\ref{lemma:d_max_concentration_whp} we have \(\dmax \lesssim \edmax\) with high probability. Together with Lemma~\ref{lemma:lp_lb} this implies \(\vallp \geq \frac{m}{\dmax} \gtrsim \frac m \edmax\).
Now, if we pick \(\tilde K \coloneqq v(k) \lesssim \frac m \edmax \), we have that \(\valbgr \lesssim \frac m \edmax\). Since \(\vallp \leq \valbgr\), we have that \(\tilde K \sim \vallp\) and \(m - F_{\tilde K} \lesssim \tilde K\). 

\subsection*{Case \(mp \gg \log n\)}
Here, we have that \(\edmax = mp (1 + o(1))\), therefore, picking an element that hits an average number of subsets is approximately the same as picking an element that hits close to maximum number of subsets. From the properties of the mean and the median of the binomial distribution, it follows that \(\Pr(\mathrm{Bin}(\tilde m, p) \geq \ceil{\tilde m p}) \geq 1/3\), for any \(\tilde m\). 

We begin with the case \(\log mp \ll \log n\). This means that \(mp\) cannot grow polynomially in \(n\), but e.g. \(mp \sim \log^2 n\) is possible. In this regime, \(\valip \sim \frac{1}{p} \log \left(\frac{mp}{\log n}\right)\). Let \(K_1 = \ceil{\frac{1}{p} \log \left(\frac{mp}{\log n}\right)}\) and \(f_1, \ldots, f_{K_1}\) be a sequence such that \(f_{s} = \ceil{mp(1-p)^s}\). Then, we have that \(m - F_{K_1} \leq m(1 - p)^{K_1} \leq \frac{1}{p}\log n\). Therefore, \((m - F_{K_1})p \sim \log n\), and we can continue with \(\tilde{f}_t\) from the previous section \(mp \sim \log n\), with \(\tilde{F}_t \coloneqq \sum_{s=1}^t \tilde{f}_t\). For this sequence \(\tilde{f}_1, \ldots, \tilde{f}_{K_2}\), we have have \(K_2 \lesssim \frac{1}{p}\), and \(m - F_{K_1} - \tilde{F}_{K_2} \lesssim \frac{1}{p} \ll \frac{1}{p} \log \left(\frac{mp}{\log n}\right)\). The required statement holds for combined sequences \(f_t\) and \(\tilde{f}_t\) and \(\tilde K \coloneqq K_1 + K_2\). \\
\noindent
Finally, we study the case \(\log mp \gtrsim \log n\), which implies that \(\valip \sim \frac{1}{p} \log n\). This case is trivial, as one can pick \(\tilde K = \ceil{\frac{1}{p} \log \left(\frac{mp}{\log n}\right)} \lesssim \valip\) and \(f_1, \ldots, f_{\tilde K}\) a sequence such that \(f_{s} = \ceil{mp(1-p)^s}\). Then, we have that \(m - F_{\tilde K} \leq m(1 - p)^{\tilde K} \leq \frac{1}{p}\log n \lesssim \valip\). 

\subsection*{From \(m - F_{\tilde K} \lesssim \tilde K\) to \(m - F_K \leq K\)}
Finally, using that \(f_t \geq 1\) by Lemma~\ref{lemma:lp_ub} unless \(F_t = m\), there exists some constant \(C > 0\), such that for \(K \coloneqq C\tilde K\), \(F_K \geq m - K\), which finishes the proof.



