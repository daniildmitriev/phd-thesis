\section{Algorithmic solutions}\label{sec:algo_sol}

\begin{algorithm}[!t]
    \caption{\bgreedy}\label{alg:block_greedy}
    \begin{algorithmic}[1]
    \State Let \(\mathcal{B}_t \subset \set{I_1,...,I_n}\) denote the \(t\)-th block, i.e. inclusion sets that {\it become} available at step \(t\).
    \State \(\mathcal{I} \gets \varnothing\)
    \State $U \gets [m]$
    \State $t \gets 0$
    \While{\(|U| > 0\) and \(\mathcal{B}_t \neq \varnothing\)}
        \State \(\mathcal{I} \gets \mathcal{I} \cup \mathcal{B}_t\) \label{step:adding_new_block} \Comment{Adding elements from the new block}
        \State \(P \gets \mathrm{argmax}_{I \in \mathcal{I}}\bigl|I \cap U\bigr| \)  \label{step:greedy_step} \Comment{Greedy step}
        \State \(\mathcal{I} \gets \mathcal{I} \setminus \{P\}\)
        \State \(U \gets U \setminus P\)
        \State \(t \gets t + 1\)
    % \doWhile
    \EndWhile
    \State \(\valbgr\ \gets t\)
    \If{\  \(|U| > 0\) \  } \quad cover the rest of \(U\) with a trivial algorithm, \(\valbgr \gets \valbgr + |U|\)
    \EndIf
    \State \Return \(\valbgr\).
    \end{algorithmic}
    \end{algorithm}

\subsection{Challenges of \(\greedy\) analysis and \(\bgreedy\) algorithm}
The aim of the present section is to conduct a rigorous analysis of the standard $\greedy$ algorithm for the \(\hs\) problem, within the prescribed Bernoulli random setting. 
In particular, we show that this routine succeeds at constructing hitting sets of optimal size w.h.p., as in the results of Section~\ref{sec:bounds}, up to multiplicative constants. 
This is done by first analysing a variation of the greedy heuristic, and subsequently proceeding by a reduction argument.\\
\noindent
% The core principle of \greedy is to construct a feasible solution in steps, by sequentially adding to the candidate solution an element which hits the largest number of remaining sets. 
% In the chosen setting, where elements are added to sets with equal probability and independently of each other, we have precise estimates on the number of subsets hit by an element which is {\it picked first}. 
% In fact, the size of this set is given by the maximum of independent Binomial random variables, which was analysed in \Cref{sec:bounds}. 
% However, this very first step introduces nontrivial dependencies amongst the remaining matrix columns and significantly complicates keeping track of the marginal gains of each subsequent element addition to the candidate solution.

% \begin{algorithm}[!t]
% \caption{\bgreedy}\label{alg:block_greedy}
% \begin{algorithmic}[1]
% \State Let \(\mathcal{B}_t \subset \set{I_1,...,I_n}\) denote the \(t\)-th block, i.e. inclusion sets that {\it become} available at step \(t\).
% \State \(\mathcal{I} \gets \varnothing\)
% \State $U \gets [m]$
% \State $t \gets 0$
% \While{\(|U| > 0\) and \(\mathcal{B}_t \neq \varnothing\)}
%     \State \(\mathcal{I} \gets \mathcal{I} \cup \mathcal{B}_t\) \label{step:adding_new_block} \Comment{Adding elements from the new block}
%     \State \(P \gets \mathrm{argmax}_{I \in \mathcal{I}}\bigl|I \cap U\bigr| \)  \label{step:greedy_step} \Comment{Greedy step}
%     \State \(\mathcal{I} \gets \mathcal{I} \setminus \{P\}\)
%     \State \(U \gets U \setminus P\)
%     \State \(t \gets t + 1\)
% % \doWhile
% \EndWhile
% \State \(\valbgr\ \gets t\)
% \If{\  \(|U| > 0\) \  } \quad cover the rest of \(U\) with a trivial algorithm, \(\valbgr \gets \valbgr + |U|\)
% \EndIf
% \State \Return \(\valbgr\).
% \end{algorithmic}
% \end{algorithm}
% \noindent
% In order to circumvent this issue, we introduce a modified greedy routine, which we refer to as the \bgreedy\ algorithm, where the elements of the ground set $[n]$ are split into separate sets of a given size, which we call blocks. 
% At the \(t\)-th iteration, the algorithm picks the element hitting the largest number of remaining sets across \textit{the first} \(t\) \textit{blocks only}. 
% By choosing the size of the blocks appropriately, we have that at each iteration $t$ one is guaranteed to find a solution of near-optimal size at least within the set of newly-included independent columns. 

% \noindent

% \bgreedy is detailed in Algorithm~\ref{alg:block_greedy}, whilst informally, it works as follows.
% \begin{enumerate}
% \item Let \(K\) be the size of the solution (suggested by theoretical analysis);
% \item Uniformly at random split \(n\) columns into \(K\) blocks with \(n / K\) columns per block;
% \item Start with an empty set of possible choices of columns;
% \item At the \(t\)-th iteration, first add the columns
% from the \(t\)-th block (Step~\ref{step:adding_new_block}).
% Then, perform one greedy step on the current
% set of possible choices (Step~\ref{step:greedy_step});
% \item If after \(K\) iterations of the algorithm, some subsets remain uncovered, we use  a trivial covering, i.e., covering each subset by a separate column.
% \end{enumerate}
% Note that the first selection of the element which hits the most number of subsets again introduces dependencies. 
% However, the columns that are in the newly added block are independent of everything else at time \(t\).

Let \(v_t\) be the element which is picked at the \(t\)-th step of \bgreedy, \(f_t\) be the number of new subsets that are hit by \(v_t\)\footnote{It may happen that \(v_t\) hits \emph{more} than \(f_t\) new subsets. In this case, we still only assume that exactly \(f_t\) are covered, and several extra sets will be covered multiple times in subsequent rounds. This overcounting simplifies the analysis and does not result in suboptimal solution.},
and \(F_t \coloneqq \sum_{i = 1}^t f_i\) be the total number of subsets which are hit after \(t\) steps.
In order to analyse how many elements \bgreedy has picked, we will consider the sequence 
\(f_1, f_2, \ldots, f_s\), with \(F_t \coloneqq \sum_{i = 1}^t f_i\), such that the following holds:
\begin{enumerate}
    \item \(F_s = m\);
    \item if \(mp \lesssim \log n\), then \(s \lesssim \vallp\), otherwise, \(s \lesssim \valip\).
\end{enumerate}

The first property ensures that \bgreedy picks at most \(s\) elements, and the second property gives optimal bounds on \(s\). 
One way to guarantee that \bgreedy succeeds is to prove that among the choices of \bgreedy at each step \(t\),
there was an element \(\tilde v_t\) which hits at least \(f_t\) new subsets w.h.p. We will prove that 
it is enough to look for \(\tilde v_t\) in 
the new block of columns \(\mathcal{B}_t\), which are added at step \(t\).
Note that unless \(F_t = m\), we have that \(f_t \geq 1\), 
since each subset is hit by at least one element w.h.p.. 
Therefore, it will be enough to find a sequence $\{f_1, f_2, \dots, f_{v}\}$ such that \(F_v \geq m - v\), since it implies \(F_{2v} = m\).
This allows us to reduce the problem of proving the effectiveness of \bgreedy to 
a key technical lemma. This lemma assumes that before step \(t\), exactly \(F_{t - 1}\)
subsets are hit, and bounds from below the probability that some vertex in the new block will hit at least \(f_t\) new subsets. 
This boils down to computing \(\Pr (\mathrm{Bin}(m - F_{t - 1}, p) \geq f_t)\).
\begin{lemma}[Informal, see~\Cref{lemma:find_one_col_lb}]
\label{lemma:find_one_col_lb_informal}
    Let $\varepsilon > 0$ and \(mp \lesssim \log n\). For some constants \(\tau > 0\), \(1 < \alpha < \beta\), and for \(t \in \mathbb{N}\), let:
    \begin{equation*}
    f_t = \left\lceil\left(\alpha / \beta \right)^k \tau\mathbb{E}\dmax\right\rceil \quad \text{where } k \text{ is such that} \quad \beta^{-k-1}m < m - F_{t-1} \leq \beta^{-k}m;
    \end{equation*}
    \noindent
    Then there exists a choice of \(\tau, \alpha, \beta\) and \(K\), such that \(F_K \geq m - K\) and  \(K \sim \vallp\). Furthermore, for this sequence \(f_t\) (which depends on \(\varepsilon\)), for any \(t \leq K\),
    \begin{equation}
        \Pr (\mathrm{Bin}(m - F_{t - 1}, p) \geq f_t) \geq n^{-\varepsilon}.
    \end{equation} 
\noindent
    Note that the implicit constants in the statements \(K \sim \vallp\) depend on \(\varepsilon\).
\end{lemma}
\noindent
This lemma highlights the crucial dependency of the problem 
on the relationship between the average degree, \(mp\), and \(\log n\). For clarity of exposition, we only state the lemma for the case \(mp \lesssim \log n\) and refer to the~\Cref{lemma:find_one_col_lb} in the Appendix for the full version and corresponding proof. Here we comment on the intuition behind the proof. 

\noindent
When \(mp \lesssim \log n\), we need to carefully track how the maximum degree changes. We look for an element which \((i)\) covers a large number of subsets, i.e., close to the expected maximum number, \(\edmax\) and \((ii)\) can be found with large enough probability. The second property is important for the reduction to the standard \greedy algorithm, whose direct analysis presents substantial difficulties, and is done later in this section. The quantity $\edmax$ is sensitive to $mp$ whenever the latter is close to $\log{n}$. Hence, we need to adjust which element we look for accordingly. This is done by setting \(f_t = \left\lceil\left(\alpha / \beta \right)^k \tau\mathbb{E}\dmax\right\rceil\) and increasing the parameter \(k\) as the number of remaining rows, \(m - F_t\), decreases. 

\noindent
For example, consider the case \(mp = \log n\). First, we can only pick a random element, since it will be as good (up to a multiplicative constant) as the maximal element. However, during the execution of the algorithm, the problem becomes more sparse, and if we continue to pick random elements, we will construct a suboptimal solution. Therefore, we gradually increase how much the newly picked element will cover, \emph{with respect to a random element}. This corresponds to the transition between Gaussian-like and Poisson-like behaviour of \(\mathrm{Bin}(m - F_{t - 1}, p)\).

\noindent
It is now straightforward to prove the following theorem, which makes rigorous the statements in Section~\ref{sec:intro}.

\begin{theorem}
    \label{thm:alg_solution_ub}
        Under Assumption~\ref{asmpt:A_np_m}, we have that
        \begin{equation}
            \begin{aligned}
                &(i) \text{ if } \  mp \lesssim \log n \  \text{then, for any } \varepsilon >0\text{ and } n \text{ large enough},\\ 
                &\qquad \Pr\left(\valbgr \lesssim \frac{m}{\mathbb{E}\dmax}\right) \geq 1 - \exp\left(-n^{1 - \delta - \varepsilon}\right); \\
                &(ii) \text{ if } \ mp \gg \log n, \text{ then, for any } \varepsilon > 0 \text{ and } n \text{ large enough}, \\
                &\qquad\Pr\left(\valbgr \lesssim \frac{1}{p} \log\left(\frac{mp}{\log n}\right)\right) \geq 1 - \exp\left(-n^{1 - \delta - \varepsilon}\right). \\
            \end{aligned}
        \end{equation}
        Note that if \(mp \gtrsim n^{\gamma}\) for some \(\gamma > 0\), then \(\log \frac{mp}{\log n} \sim \log n\), and the bound in \((ii)\) can be simplified.
\end{theorem}
\begin{proof}
The main idea of the proof is to analyse the distribution of the columns that are added at each step \(t\). These columns are independent, and for each newly added column, the number of additional subsets which it covers is distributed according to \(\mathrm{Bin}(m - F_{t - 1}, p)\), where \(F_{t - 1}\) is the number of subsets which are already covered. Lemma~\ref{lemma:find_one_col_lb} (see~\Cref{lemma:find_one_col_lb_informal} above for an informal version) allows us to lower bound \(F_t\), and we show now that we can do this with high probability.

\noindent
Fix \(\varepsilon > 0\) and let \(\varepsilon' \coloneqq \varepsilon / 4\). Let \(f_1, f_2, \ldots\) be the sequence from Lemma~\ref{lemma:find_one_col_lb} for \(\varepsilon'\) and let \(K\) be the value for which (\ref{eq:cover_size}) is satisfied, i.e. \(F_{K} \geq m - K\). Notice that $K \leq C \max\left\{ \frac{m}{\E \dmax}, \frac{1}{p} \log(\frac{mp}{\log{n}}) \right\}$ for some constant $C > 0$, for \(n\) large enough. We uniformly at random split \(n\) elements (columns) into \(K\) groups of size \(n / K\) each (assuming without loss of generality that $K$ divides $n$, otherwise we consider groups of size \(\floor{n / K}\)), so that $\mathcal{B}_t$ yields a new set of $n/K$ elements at each iteration $t \leq K$ and \(\mathcal{B}_t = \varnothing\) for \(t > K\). We say that the algorithm fails at step \(t\) if before step \(t\), at least \(F_{t - 1}\) subsets are covered, but after step \(t\) less than \(F_t\) sets are covered. 
Using that, for \(n\) large enough, $(i)$ columns in each newly added block are independent, \((ii)\) \(\Pr \left(\mathrm{Bin}(m - F_{t - 1}, p) \geq f_t\right) \geq n^{-\varepsilon'}\), and \((iii)\) \(n / K \geq n^{1 - \delta - \varepsilon'}\), we get
\begin{align*}
\Pr\left(\text{\bgreedy fails at step t} \right) &\overset{(i)}\leq \left(\Pr\left(\mathrm{Bin}(m - F_{t - 1}, p) < f_t\right) \right)^{n / K} \\
&\overset{(ii)}{\leq} \left( 1 - n^{-\varepsilon'} \right)^{n / K} \\
& \overset{(iii)}{\leq} \exp\left( - n^{1 - \delta - 2\varepsilon'}\right).
\end{align*}
We then proceed by applying a union bound to obtain the result,
\begin{align*}
&\Pr \left(\bgreedy\text{ fails during first } K \text{ steps}\right) \\
&\quad \leq \sum_{t=1}^K \Pr\left( \bgreedy\text{ fails at step t}\right) \\
&\quad \leq K \cdot \exp\left(- n^{1 - \delta - 2\varepsilon'}\right) \\
&\quad \leq \exp\left(- n^{1 - \delta - 3\varepsilon'} \right),
\end{align*}
where the second inequality holds since, by definition, the algorithm runs for $K$ iterations, and the third one holds for $n$ large enough. 
We proved that \bgreedy succeeds in finding at most $K$ elements such that at most $m - F_{K}$ sets remain uncovered. Since by construction, $m - F_{K} \leq K$, we can cover the remaining rows trivially using that \(\ip\) is feasible by Lemma~\ref{lemma:lp_ub} with high probability, which proves that 
\begin{equation*}
    \Pr \left(\valbgr \leq 2K\right) \geq 1 - \exp\left(-n^{1 - \delta - 4\varepsilon'}\right) =1 - \exp\left(-n^{1 - \delta - \varepsilon}\right) ,
\end{equation*}
for $n$ large enough. Recalling that \(K \lesssim \vallp\) for \(mp \lesssim \log n\), and that \(K \lesssim \valip\) for \(mp \gg \log n\), finishes the proof. 
\end{proof}

\begin{corollary}
\label{cor:ipgap_collected}
    Under~\Cref{asmpt:A_np_m}, we have that for any \(D > 0\),
    \begin{equation}
            \begin{aligned}
                &(i) \text{ for any } n \text{ large enough},\\ 
                &\qquad \Pr\left(\valbgr \sim \valip\right) \geq 1 - n^{-D}; \\
                &(ii) \text{ if } \  mp \lesssim \log n, \  \text{then, for any } n \text{ large enough},\\ 
                &\qquad \Pr\left(\ipgap \sim 1\right) \geq 1 - n^{-D}; \\
                &(iii) \text{ if } \ mp \gg \log n, \text{ then, for any } n \text{ large enough}, \\
                &\qquad\Pr\left(\ipgap \sim \log \left(\frac{mp}{\log n}\right)\right) \geq 1 - n^{-D}. \\
            \end{aligned}
        \end{equation}
\end{corollary}
\begin{proof}
Proof follows from~\Cref{lemma:lp_lb_ub},~\Cref{lemma:first_moment_mp_large}, and~\Cref{thm:alg_solution_ub}.
\end{proof}

\subsection{Reduction from \(\bgreedy\) to \(\greedy\)}
With the above results at hand, we now proceed to analyse the \(\greedy\) algorithm by means of a suitable reduction. Recall that we denote outputs of \(\bgreedy\) and \(\greedy\) as \(\valbgr\) and \(\valgr\) respectively. 
\begin{theorem} \label{thm:greedy}
Under Assumption \ref{asmpt:A_np_m} with $\delta < 1/2$, we have that, for \(n\) large enough,
\begin{equation*}
    \Pr\left(\valgr \sim \valip \right) \geq 1 - \exp\left(-\sqrt{n}\right).
\end{equation*}
\end{theorem}
%
\begin{proof}
 We use Theorem~\ref{thm:alg_solution_ub} with \(\varepsilon = 1/8 - \delta / 4\), and let $K, \mathcal{B}_t$ be as defined in the proof of Theorem~\ref{thm:alg_solution_ub}.  
We have that, for \(n\) large enough,
\begin{align*}
\Pr \left(\bgreedy\text{ fails at any step}\right) \leq \exp\left(-n^{\Delta} \right),
\end{align*}
where \(\Delta \coloneqq 3/4 - \delta / 2 > 1/2\). 

\noindent
Given a matrix $\A$, consider running the above definition of \bgreedy for $J := \exp(\sqrt{n})$ times, each time reshuffling the columns. In what follows, we address \(\bgreedy\) and \(\greedy\) defined with the same tie-breaking strategy when it comes to a number of elements hitting the same number of sets, i.e., selecting the left-most column in the associated matrix $\A$. Both \(\valbgr\) and \(\valgr\) are random variables, but conditioned on $\A$, \(\valgr\) is deterministic, while \(\valbgr\) still depends on the randomness of separating columns into blocks.
Using the union bound, we have that
\begin{equation}
\begin{aligned}
\label{eq:gr_first_ub}
\Pr\br{\valgr > 2K } \leq 
\ &\Pr\br{\exists\text{ a failed copy of }\bgreedy} \\
 &+ \Pr\br{\valbgr < \valgr \text{ over all $J$ copies} }.
\end{aligned}
\end{equation}
Applying the union bound again, we can upper bound the first term in~(\ref{eq:gr_first_ub}):
\begin{equation}
\label{eq:gr_second_ub}
    \Pr\left(\exists\text{ a failed copy of \bgreedy}\right) \leq J \exp\left(- n^{\Delta} \right) = \exp\left(-n^{\Delta} + n^{1/2} \right).
\end{equation}
Now we focus on the second term in~(\ref{eq:gr_first_ub}).
Let \(v_1, v_2, \ldots, v_g\) be the ordered sequence of elements picked by \(\greedy\). 
Let \(M_t \coloneqq \{v_1 \in \mathcal{B}_1, v_2 \in \mathcal{B}_1 \cup \mathcal{B}_2, \ldots, v_{t} \in  \mathcal{B}_1 \cup \ldots \cup \mathcal{B}_t\}\).
The event \(\{\valbgr \geq \valgr\}\) contains the event \(M_g\), since in this case \(\bgreedy\) will necessarily pick exactly the same columns \(v_1, v_2, \ldots, v_g\). Given that each reshuffling of the columns generates a uniform distribution of \(\mathcal{B}_i\)'s over possible partitions of \(n\) columns, we get that
\begin{align*}
    \Pr\left(M_g\right) = \prod_{t=1}^g \Pr\left(\v_t \in \mathcal{B}_1 \cup \ldots \cup \mathcal{B}_t \mid M_{t - 1}\right)   
 \end{align*}
The \(t\)-th term in the product above is equal to  \begin{equation*}
    \Pr(\v_t \in \mathcal{B}_1 \cup \ldots \cup \mathcal{B}_t \mid M_{t - 1}) =  \frac{t \frac{n}{K} - (t - 1)}{n - (t - 1)} \geq \frac{t}{K} - \frac{t - 1}{n} \geq \frac{t}{2(K - 1)},
\end{equation*}
where the last inequality holds for \(n \geq 4K\) (recall that \(n \gg K\)). Since \(M_g \subset \{\valbgr \geq \valgr\}\), we can lower bound the probability of the latter event as follows (note that when \(g < K\) there will be less terms in the product, hence, \(\Pr(M_g)\) will be even larger),
\begin{align*}
&\Pr\left(\valbgr \geq  \valgr \text{ for } 1 \text{ copy}\right) \geq \Pr(M_g) \\
&\quad \geq \prod_{t=1}^{K - 1} \Pr(\v_t \in \mathcal{B}_1 \cup \ldots \cup \mathcal{B}_t \mid M_{t - 1})  \geq \prod_{t=1}^{K - 1} \frac{t}{2(K - 1)} \geq e^{-2K},
\end{align*}
where we used that \(k! \geq (k/e)^k\) in the last inequality.
Since \(K \leq C \max\left\{ \frac{m}{\E \dmax}, \frac{1}{p} \log(\frac{mp}{\log{n}}) \right\}\) and \(1/p \leq n^{\delta}\), there exists a constant \(\tilde C > 0\) large enough, such that \(K  \leq \tilde C n^{\delta} \log n\). Therefore, using independence of the reshuffling between the copies, we can compute
\begin{equation}
\label{eq:gr_third_ub}
\begin{aligned}
\Pr\left(\valbgr < \valgr \text{ over all $J$ copies}   \right)& = \left( 1 - \Pr\left(\valbgr \geq  \valgr \text{ for } 1 \text{ copy} \right) \right)^J \\
& \leq (1 - e^{-2K})^J \\
& \leq \exp\left(- e^{\sqrt{n}- 2\tilde C n^{\delta} \log{n}} \right).
\end{aligned}
\end{equation}
%
Combining (\ref{eq:gr_first_ub}), (\ref{eq:gr_second_ub}) and (\ref{eq:gr_third_ub}), we showed that $\Pr\left(\valgr > 2K \right) \leq \exp\left(-\sqrt{n}\right)$ for \(n\) large enough, which finishes the proof.  
\end{proof}

\begin{remark}
We note that the $\delta < 1/2$ condition in Theorem~\ref{thm:greedy} is likely not optimal, and could be relaxed by reducing to \bgreedy with more carefully chosen sets $\mathcal{B}_t$. In particular, the appropriate set sizes $|\mathcal{B}_t|$ may not be identical across $t \leq K$. The analysis becomes more technical in this case, and we highlight this as an interesting open direction.
\end{remark}